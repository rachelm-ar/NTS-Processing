{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc584127",
   "metadata": {},
   "source": [
    "# WP-2 mdltour warning checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad3d61",
   "metadata": {},
   "source": [
    "<p style=\"color:rgb(0,162,219); font-family:Arial; font-size:16px;\">Notebook Information </p>\n",
    "\n",
    "<table style=\"color:rgb(88,89,91); font-family:Arial; float:left; font-size:13px; text-align:left;\">\n",
    "    <tr>\n",
    "        <td style=\"color:rgb(0,90,132);font-size:13px; text-align:left;\"><b>Project</b></td>\n",
    "        <td style=\"text-align:left;\">NorMITs Demand Partner </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"color:rgb(0,90,132);font-size:13px; text-align:left;\"><b>Primary Contact Name</b></td>\n",
    "        <td style=\"text-align:left;\"> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"color:rgb(0,90,132);font-size:13px; text-align:left;\"><b>Primary Contact Email</b></td>\n",
    "        <td style=\"text-align:left;\"> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"color:rgb(0,90,132);font-size:13px; text-align:left;\"><b>Document Sensitivity</b></td>\n",
    "        <td style=\"text-align:left;\"> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa976bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from utils import ChecksLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81d78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = ChecksLogger(log_file_name='checks_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2709921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths etc.\n",
    "model_version = 'v5'\n",
    "known_correct_schema_model_version = 'v3' # Version of the tour model which we know has the correct schema in its output files for comparing new runs to\n",
    "\n",
    "tour_dir = r'I:\\NTS\\outputs\\tour'\n",
    "reports_dir = os.path.join(tour_dir, 'reports', model_version)\n",
    "known_correct_schema_dir = os.path.join(\n",
    "    tour_dir,\n",
    "    'reports',\n",
    "    known_correct_schema_model_version\n",
    ")\n",
    "\n",
    "mat_file = 'matrix_county_output.csv'\n",
    "report_files = ['attr_county_output.csv', \n",
    "                mat_file, \n",
    "                'nts_distr_output.csv', \n",
    "                'nts_mts_output.csv', \n",
    "                'nts_tour_output.csv', \n",
    "                'prod_county_output.csv']\n",
    "\n",
    "script_cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7e0cc",
   "metadata": {},
   "source": [
    "### TM011 - Check expected output files exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c02d7b",
   "metadata": {},
   "source": [
    "Check specified output files have been produced in expected location and contain data, have been last edited within a certain time window of the initial tour model script running (which is hopefully logged somewhere?) - in other words, checking these are definitely the outputs produced by the latest run. Probably wants to be the first thing we check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2174c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output_files(report_files, reports_dir, time_window):\n",
    "    \"\"\"\n",
    "    Check if a file exists, has data, and has been modified within time window.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    report_files: list of strings\n",
    "        List of expected tour model output files\n",
    "    reports_dir: str (file path)\n",
    "        Directory for expected tour model output files\n",
    "    time_window: datetime object\n",
    "        Acceptable time window to be considered output from latest run\n",
    "    \n",
    "    Outputs\n",
    "    -----------\n",
    "    None, writes outcome of checks straight to log file\n",
    "    \"\"\"\n",
    "    checkstring = f\"TM011: Check output files exist\"\n",
    "    logger.info(checkstring, f\"Commencing check TM011:\")\n",
    "    for file in report_files:\n",
    "        file_path = os.path.join(reports_dir, file)\n",
    "        # Check if file exist\n",
    "        if os.path.exists(file_path):\n",
    "            # Check if file has data\n",
    "            if os.path.getsize(file_path) > 0:\n",
    "                # Check if file has been modified within the time window\n",
    "                # TODO: Work out a minimum time and implement that.\n",
    "                # Is there an existing run log this could be obtained from?\n",
    "                mod_time = os.path.getmtime(file_path)\n",
    "                current_time = time.time()\n",
    "                if current_time - mod_time <= time_window:\n",
    "                    logger.success(checkstring, f\"Success - {file} exists and contains data from latest run.\")\n",
    "                else:\n",
    "                    logger.warning(checkstring, f\"Warning - {file} exists and contains data, but was not produced by latest run.\")\n",
    "            else:\n",
    "                logger.warning(checkstring, f\"Warning - {file} exists but does not contain data.\")\n",
    "        else:\n",
    "            logger.warning(checkstring, f\"Warning - {file} does not exist.\")\n",
    "    logger.info(checkstring, f\"Check TM011 complete\")\n",
    "    logger.save_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508209f",
   "metadata": {},
   "source": [
    "### TM012 - Check output schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b6921",
   "metadata": {},
   "source": [
    "List the expected column names and schemas in all output files, then check the outputs match these. Might end up wrapping some of the other checks into this depending on things like table shapes and how descriptive Python schemas are. Probably also wants to be quite an early check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65082ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_datatypes(csv_file):\n",
    "    \"\"\"\n",
    "    Get the columns and data types of a CSV file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_file: str (file path)\n",
    "        Path to CSV file to check\n",
    "    \n",
    "    Outputs:\n",
    "    -----------\n",
    "    columns_datatypes: list\n",
    "        List of column datatypes for the input CSV file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    columns_datatypes = {column: str(df[column].dtype) for column in df.columns}\n",
    "    return columns_datatypes\n",
    "\n",
    "# def process_csv_files(report_files, reports_dir, script_cwd):\n",
    "def find_correct_schemas(report_files, known_correct_schema_dir, script_cwd, checkstring):\n",
    "    \"\"\"\n",
    "    Find the expected schemas of tour model ouput files\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    report_files: list of strings\n",
    "        List of expected tour model output files\n",
    "    known_correct_schema_dir: str (file path)\n",
    "        Directory containing tour model output files which are known to have the correct schema\n",
    "    script_cwd: str (path)\n",
    "        Location the script is currently running.\n",
    "        JSON files will be dumped here during running\n",
    "    checkstring: str\n",
    "        String identifying the check that has called this function\n",
    "    \n",
    "    Outputs:\n",
    "    -----------\n",
    "    dumps_path: str (filepath)\n",
    "        Path where the expected schemas are stored\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for f in report_files:\n",
    "        if f[-4:] == '.csv':\n",
    "            file_path = os.path.join(known_correct_schema_dir, f)\n",
    "            if os.path.exists(file_path):\n",
    "                result[f] = get_columns_datatypes(file_path)\n",
    "            else:\n",
    "                result[f] = \"File not found\"\n",
    "                logger.warning(\n",
    "                    checkstring,\n",
    "                    f\"Warning - {f} not present in directory that should contain all the expected outputs with the correct schemas.\"\n",
    "                )\n",
    "                logger.warning(\n",
    "                    checkstring,\n",
    "                    f\"Are you sure {known_correct_schema_dir} is the correct directory for this?\"\n",
    "                )\n",
    "    \n",
    "    dumps_path = os.path.join(script_cwd, 'checks_json_dump_temp')\n",
    "    if not os.path.exists(dumps_path):\n",
    "        os.makedirs(dumps_path)\n",
    "    json_filename = os.path.join(dumps_path, 'columns_datatypes.json')\n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json.dump(result, json_file, indent=4)\n",
    "    logger.save_logs()\n",
    "    return json_filename\n",
    "\n",
    "def compare_with_json(json_filename, checkstring, folder_path, expected_files):\n",
    "    \"\"\"\n",
    "    Compare JSON output with new CSV files to find and report any issues with schemas\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    json_filename: str (filepath)\n",
    "        Path to find the json file which contains the expected schema inforamtion\n",
    "    checkstring: str\n",
    "        String identifying the check that has called this function\n",
    "    folder_path: str (filepath)\n",
    "        Directory containing the tour model outputs\n",
    "    expected_files: list of strings\n",
    "        Files that should exist in the tour model outputs\n",
    "    \n",
    "    Outputs:\n",
    "    -----------\n",
    "    column_errors: list\n",
    "        List of errors resulting from mismatched columns\n",
    "    datatype_errors: list\n",
    "        List of cases where field datatypes are not as expected\n",
    "    matching_files: list\n",
    "        List of all files that match the expected schemas\n",
    "    \"\"\"\n",
    "    column_errors = []\n",
    "    datatype_errors = []\n",
    "    matching_files = []\n",
    "    \n",
    "    if not os.path.isfile(json_filename):\n",
    "        # If json containing expected schemas does not exist, try to create it\n",
    "        logger.info(\n",
    "            checkstring,\n",
    "            f\"Info - Specified location for json expected schema file {json_filename} did not exist, now trying to create it...\"\n",
    "        )\n",
    "        json_filename = find_correct_schemas(report_files, known_correct_schema_dir, script_cwd, checkstring)\n",
    "        skip2check = False\n",
    "    else:\n",
    "        skip2check = True\n",
    "    if skip2check == False or not os.path.exists(json_filename):\n",
    "        # If the json file is still missing, create warning\n",
    "        logger.warning(\n",
    "            checkstring,\n",
    "            f\"Warning - Even after trying to create {json_filename} it still does not exist!\"\n",
    "        )\n",
    "        logger.warning(\n",
    "            checkstring,\n",
    "            f\"Warning - {checkstring} cannot be completed!\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        with open(json_filename, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        for file_name, expected_columns in json_data.items():\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if not os.path.exists(file_path):\n",
    "                logger.warning(\n",
    "                    checkstring,\n",
    "                    f\"Warning - {file_name}, which is expected to exist, is not found in the specified folder.\"\n",
    "                )\n",
    "            else:\n",
    "                actual_columns = get_columns_datatypes(file_path)\n",
    "                if expected_columns.keys() != actual_columns.keys():\n",
    "                    column_errors.append(file_name)\n",
    "                else:\n",
    "                    if all(column in actual_columns and actual_columns[column] == datatype\n",
    "                           for column, datatype in expected_columns.items()):\n",
    "                        matching_files.append(file_name)\n",
    "                    else:\n",
    "                        for column, datatype in expected_columns.items():\n",
    "                            if column not in actual_columns or actual_columns[column] != datatype:\n",
    "                                datatype_errors.append((file_name, column))\n",
    "\n",
    "    logger.save_logs()\n",
    "    return column_errors, datatype_errors, matching_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2455c9d",
   "metadata": {},
   "source": [
    "### TM005 - Intrasector trips should be highest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d6132",
   "metadata": {},
   "source": [
    "The maximum value for any given row/column in the output matrix is expected to be the intrasector cell. Allow some leeway here though - exception is probably Air travel as a mode, and rail in some instances. Calculate % average, flag if not inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d0a366d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_intras(reports_dir, mat_file, threshold):\n",
    "    \"\"\"\n",
    "    Check if the intra-county cell is the highest for each\n",
    "    mode, purpose, direction, period combination\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    reports_dir: str (path)\n",
    "        Directory containing the tour model output matix file\n",
    "    mat_file: str (filename)\n",
    "        Name of the tour model output matrix file\n",
    "    threshold: float\n",
    "        Proportion of the intra-county value above the\n",
    "        intra-county value that the script will allows as a\n",
    "        tolerance before flagging a warning\n",
    "    \n",
    "    Outputs:\n",
    "    ----------\n",
    "    None, writes warnings to log files\n",
    "    \"\"\"\n",
    "    logger.info(checkstring, f\"Commencing check TM005:\")\n",
    "    \n",
    "    tour_mat = pd.read_csv(os.path.join(reports_dir, mat_file))\n",
    "    md = tour_mat['mode'].unique()\n",
    "    pr = tour_mat['purpose'].unique()\n",
    "    di = tour_mat['direction'].unique()\n",
    "    tp = tour_mat['period'].unique()\n",
    "    og = tour_mat['tmz_o'].unique()\n",
    "\n",
    "    n=0\n",
    "    checkstring = 'TM005: Intrasector trips should be highest'\n",
    "\n",
    "    for m in md:\n",
    "        for p in pr:\n",
    "            for dr in di:\n",
    "                for t in tp:\n",
    "                    tour_mat_filtered = tour_mat[\n",
    "                        (tour_mat['mode'] == m) &\n",
    "                        (tour_mat['purpose'] == p) &\n",
    "                        (tour_mat['direction'] == dr) &\n",
    "                        (tour_mat['period'] == t)\n",
    "                    ]\n",
    "                    tour_mat_filtered = tour_mat_filtered.drop(\n",
    "                        columns=[\n",
    "                            'mode',\n",
    "                            'purpose',\n",
    "                            'direction',\n",
    "                            'period'\n",
    "                        ])\n",
    "                    for o in og:\n",
    "                        # Only working 1 dimensionally to avoid duplicate warning in some cases\n",
    "                        # I.e. not repeating this in repacing all o's with d's and vice versa\n",
    "                        # Already generates many warnings!\n",
    "                        tour_mat_o = tour_mat_filtered[\n",
    "                            (tour_mat_filtered['tmz_o'] == o)\n",
    "                        ]\n",
    "                        tour_mat_intra = tour_mat_o[\n",
    "                            (tour_mat_o['tmz_d'] == o)\n",
    "                        ].trips.sum()\n",
    "\n",
    "                        # Filter out cases where the mode is not long distance (i.e. air and rail) and \n",
    "                        # there is a small matrix total, then warn where the intrasectors are 0.\n",
    "                        if (m != 'Air') & (m != 'Rail') & (tour_mat_o.trips.sum() > 100) & (tour_mat_intra <= 0):\n",
    "                            logger.warning(\n",
    "                                checkstring,\n",
    "                                f'Warning - No {p} {m} {dr} intras in county: {o} in time period {t}'\n",
    "                            )\n",
    "                        elif(tour_mat_intra > 0):\n",
    "                            tour_mat_max = tour_mat_o.trips.max()\n",
    "                            if tour_mat_intra < tour_mat_max: \n",
    "                                for d in og:\n",
    "                                    d_trips = tour_mat_o[(tour_mat_o['tmz_d'] == d)].trips.sum()\n",
    "                                    prop = d_trips/tour_mat_intra\n",
    "                                    if prop > (1 + threshold):\n",
    "                                        logger.warning(\n",
    "                                            checkstring,\n",
    "                                            f\"Warning - for {p} {m} {dr} trips in time period {t}, {o} {d} trips are {prop*100:.2f}% of the intra-county trips in {o}\"\n",
    "                                        )\n",
    "\n",
    "                        if n % 10000 == 0:\n",
    "                            print(n) # Show progress is occuring whilst running\n",
    "                        n = n + 1\n",
    "    logger.info(checkstring, f\"Check TM005 complete\")\n",
    "    logger.save_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b25c8516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check TM011 complete and logged\n",
      "Check TM012 complete and logged\n",
      "Check TM005 commencing. This can take a while to run\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "Check TM005 complete and logged\n"
     ]
    }
   ],
   "source": [
    "# CELL TO CALL ALL OF THE ABOVE CHECKS\n",
    "\n",
    "# Check TM011 - Check expected output files exist\n",
    "time_window = time.time() # Confirm time frame\n",
    "time_window\n",
    "check_output_files(report_files, reports_dir, time_window)\n",
    "print('Check TM011 complete and logged') # For info when running\n",
    "\n",
    "# Check TM012 - Check output schemas\n",
    "tm012 = 'TM012: Check output schemas'\n",
    "logger.info(tm012, f\"Commencing check TM012:\")\n",
    "json_file = find_correct_schemas(report_files, known_correct_schema_dir, script_cwd, tm012)\n",
    "column_errors, datatype_errors, matching_files = compare_with_json(json_file, tm012, reports_dir, report_files)\n",
    "    \n",
    "if column_errors:\n",
    "    for i in column_errors:\n",
    "        logger.warning(\n",
    "            tm012,\n",
    "            f\"Warning - Column errors found in {i}\"\n",
    "        )\n",
    "\n",
    "if datatype_errors:\n",
    "    for file_name, column in datatype_errors:\n",
    "        logger.warning(\n",
    "            tm012,\n",
    "            f\"Warning - Data type errors found in {file_name}: {column}\"\n",
    "        )\n",
    "\n",
    "if matching_files:\n",
    "    for i in matching_files:\n",
    "        logger.success(\n",
    "            tm012,\n",
    "            f\"Sucsess - {i} matches the expected columns and data types\"\n",
    "        )\n",
    "\n",
    "# if not column_errors and not datatype_errors and not matching_files:\n",
    "#     for i in report_files:\n",
    "#         print(i, \" file matches the expected columns and data types.\")\n",
    "logger.info(tm012, f\"Check TM012 complete\")\n",
    "logger.save_logs()\n",
    "print('Check TM012 complete and logged') # For info when running\n",
    "\n",
    "# Check TM005 - Intrasector trips should be highest\n",
    "print('Check TM005 commencing. This can take a while to run') # For info when running\n",
    "check_intras(reports_dir, mat_file, 0.1)\n",
    "print('Check TM005 complete and logged') # For info when running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aecfa1",
   "metadata": {},
   "source": [
    "### TM001 - Check tour model outputs non-zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685774b2",
   "metadata": {},
   "source": [
    "Check that the total in the matrix output file is > 0\n",
    "Also check each row and column in the output matrix sums to a > 0 value (will cover some of the checks to ensure everything is there too - rest of this check covered by TM010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "830f6b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns and rows sum to greater than 0\n"
     ]
    }
   ],
   "source": [
    "dest_col_sums = pivot_table.sum(axis=0)\n",
    "origin_col_sums = pivot_table.sum(axis=1)\n",
    "    \n",
    "try:\n",
    "    if (dest_col_sums <= 0).any():\n",
    "        probl_col = pivot_table.columns[dest_col_sums <= 0]\n",
    "        raise ValueError(f'No value in destination column: {probl_col}')\n",
    "        \n",
    "    if (origin_col_sums <= 0).any():\n",
    "        probl_row = pivot_table.index[origin_col_sums <= 0]\n",
    "        raise ValueError(f'No value in origin row: {probl_row}')\n",
    "        \n",
    "    print('All columns and rows sum to greater than 0')\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f'Error: {e}, Column sum: {dest_col_sums}, Row sum: {origin_col_sums}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124cfbc1",
   "metadata": {},
   "source": [
    "### TM002 - Check tour model outputs not negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81b2ec",
   "metadata": {},
   "source": [
    "Check that no cell in the matrix output is < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dfa8206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No negative value in model output\n"
     ]
    }
   ],
   "source": [
    "# Function to check for negative values and raise an error with details\n",
    "def check_negative_values(pivot_table):\n",
    "    errors = []\n",
    "    for r in pivot_table.index:\n",
    "        for c in pivot_table.columns:\n",
    "            if pivot_table.at[r, c] < 0:\n",
    "                origin_id, origin_name = r\n",
    "                destination_id, destination_name = c\n",
    "                errors.append(f\"Negative value found at Origin: {origin_name} (ID: {origin_id}), Destination: {destination_name} (ID: {destination_id})\")\n",
    "\n",
    "    if errors:\n",
    "        raise ValueError(\"Errors found:\\n\" + \"\\n\".join(errors))\n",
    "\n",
    "# Run the function to check for negative values\n",
    "try:\n",
    "    check_negative_values(pivot_table)\n",
    "    print('No negative value in model output')\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bd5c23",
   "metadata": {},
   "source": [
    "### TM003 - Check for NULLs/NaNs in tour model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a4980",
   "metadata": {},
   "source": [
    "Check for NULLs/NaNs in tour model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab1383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No null value in model output\n"
     ]
    }
   ],
   "source": [
    "def check_null_values(pivot_table):\n",
    "    errors = []\n",
    "    for r in pivot_table.index:\n",
    "        for c in pivot_table.columns:\n",
    "            if pd.isnull(pivot_table.at[r, c]):\n",
    "                origin_id, origin_name = r\n",
    "                destination_id, destination_name = c\n",
    "                errors.append(f\"Null value found at Origin: {origin_name} (ID: {origin_id}), Destination: {destination_name} (ID: {destination_id})\")\n",
    "\n",
    "    if errors:\n",
    "        raise ValueError(\"Errors found:\\n\" + \"\\n\".join(errors))\n",
    "\n",
    "# Run the function to check for null values\n",
    "try:\n",
    "    check_null_values(pivot_table)\n",
    "    print('No null value in model output')\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9802776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN value in model output\n"
     ]
    }
   ],
   "source": [
    "def check_null_values(pivot_table):\n",
    "    errors = []\n",
    "    for r in pivot_table.index:\n",
    "        for c in pivot_table.columns:\n",
    "            if pd.isna(pivot_table.at[r, c]):\n",
    "                origin_id, origin_name = r\n",
    "                destination_id, destination_name = c\n",
    "                errors.append(f\"NaN value found at Origin: {origin_name} (ID: {origin_id}), Destination: {destination_name} (ID: {destination_id})\")\n",
    "\n",
    "    if errors:\n",
    "        raise ValueError(\"Errors found:\\n\" + \"\\n\".join(errors))\n",
    "\n",
    "# Run the function to check for NaN values\n",
    "try:\n",
    "    check_null_values(pivot_table)\n",
    "    print('No NaN value in model output')\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93970bf9",
   "metadata": {},
   "source": [
    "### TM010 - Check zones/sectors in output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8af590",
   "metadata": {},
   "source": [
    "Check list of zones/sectors in output against a list of zones we expect to see - will account for any zones/sectors that have been dropped entirely rather than just 0-ed (as tested for by TM001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
