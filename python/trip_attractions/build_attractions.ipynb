{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook to eventually run all the regression modelling process (exlcudes NTS data prcoessing)\n",
    "v0.1 - 13/05/2024, JN - Originated\n",
    "v0.2 - 13/05/2024, ART - Checking and switch to Ridge model\n",
    "     - 14/05/2024, ART - Further checking\n",
    "v0.3 - 15/05/2024, ART - Adding further input control and education data\n",
    "     - 16/05/2024, ART - Picking up changes made in (forked) v0.2 version late on 15/05 (output renaming)\n",
    "     - 16/05/2024, ART - Completed implementation of education, added switch to turn purposes on and off\n",
    "     - 16/05/2024, ART - Picked up further file name corrections from the (forked) v0.2 version following CLFL comments\n",
    "v0.3_M6.18p - 16/05/2024, ART - Setting up to run Model 6.18 in python - Commute/Employer's Business only\n",
    "v0.3_M6.19p - 16/05/2024, ART - Setting up to run Model 6.19 in python - Employer's Business only\n",
    "v0.3_M6.20p - 16/05/2024, ART - Setting up to run Model 6.20 in python - Shopping only\n",
    "v0.3_M6.21p - 16/05/2024, ART - Setting up to run Model 6.20 in python - Personal Business only\n",
    "v0.4 - 16/05/2024, ART - Altering selection conditions to allow single notebook to run all purposes 'finalised' so far\n",
    "     - 20/05/2024, ART - More SOC selection condition improvements\n",
    "                         Allows purpose-based selection of Area Type Grouping\n",
    "                         Allows multiple SIC codes to be grouped to a single coefficient (per Area Type Grouping)\n",
    "v0.5 - 21/05/2024, ART - Adding Visit Friends purpose using Household data, renaming AT grouping to versioned options\n",
    "v0.6 - 21/05/2024, ART - Debugging education when aggregating with the pupil-based data\n",
    "                         Correcting SIC/Education data join that was working incorrectly\n",
    "                         Setting education LSOA to MSOA correspondences to better match what is in the datasets\n",
    "                         Note there appears to be issues with the Wales Further Education Data at source\n",
    "v0.7 - 22/05/2024, ART - Additional output file - all coeffs in one file\n",
    "                         Fields: Purpose (1-8), Area type grouping, SOC(named combination/all), SIC code, Coefficient\n",
    "                         General tidying\n",
    "v0.8 - 23/05/2024, ART - Reformatting combined coefficient output file to\n",
    "                         Fields: Purpose (1-8), Area Type (1-20), SOC(1-4), SIC code/Data, Coefficient\n",
    "TRANSFER TO GIT AND RENAMED FROM: build_attractions_Fullpipeline_v0.8.ipynb\n",
    "                              TO: build_attractions.ipynb\n",
    "                              ART, 23/05/2024 - QA beyond this point in Git commit messages\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd729135",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc62126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "import sklearn\n",
    "from scipy.optimize import minimize, rosen, rosen_der\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8b3d3",
   "metadata": {},
   "source": [
    "# Model Details and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name\n",
    "model_name = 'TestingPython'\n",
    " \n",
    "# Model version\n",
    "model_version = 'output'\n",
    " \n",
    "# NTS data direction selection\n",
    "nts_direction = 'nhb' # Can be 'hb_fr' OR 'nhb'\n",
    "\n",
    "# Model directory\n",
    "modeldirname = '_'.join([nts_direction, 'output_attractions', model_name, model_version])\n",
    " \n",
    "# Select which purposes to run for (overwrites everything below)\n",
    "    # Options are \"On\" or \"Off\"\n",
    "    # If not \"On\" then purpose dropped\n",
    "purpose_inclusion = {\n",
    "    \"Commute\": \"On\",\n",
    "    \"Employer's Business\": \"On\",\n",
    "    \"Education\": \"On\",\n",
    "    \"Shopping\": \"On\", \n",
    "    \"Personal Business\": \"On\",\n",
    "    \"Social\": \"On\",\n",
    "    \"Visit Friend\": \"On\",\n",
    "    \"Holiday\": \"On\"\n",
    "}\n",
    " \n",
    "# Set the Area Type classifcations by purpose\n",
    "    # - Note if a purpose is not assigned an Area Type here, it is dropped!\n",
    "    # - Each Area Type classifcation here must be defined in a cell further down\n",
    "    # - If an acitve purpose is listed twice, the output will be generated once for each AT classification system it is in\n",
    "    # - Active purposes listed twice will generate a warning, as will active purposes not listed at all\n",
    "    # - Each Area Type grouping is assigned a list of purposes\n",
    "    # - If there are no purposes for a grouping, the list should be empty\n",
    "at_purp_classifcation = {\n",
    "    \"none\": [],\n",
    "    \"grouped_v1\": [],\n",
    "    \"grouped_v2\": [\"Commute\", \"Employer's Business\", \"Education\", \"Personal Business\", \"Visit Friend\"],\n",
    "    \"grouped_v3\": [\"Shopping\", \"Social\"],\n",
    "    \"grouped_v4\": [],\n",
    "    \"grouped_v5\": [\"Holiday\"],    \n",
    "    \"individual\": []\n",
    "}\n",
    " \n",
    "# SOC selection for purposes that require it.\n",
    "    # - Assume this will just be Commute/Employer's Business\n",
    "    # - Some purposes like education and visit friends may not behave as expected if incuded here\n",
    "    # - ONLY include purposes where SOC selection is required\n",
    "    # - Allowed values 1 to 4\n",
    "    # - Ints in a list will output results by individual SOC\n",
    "    # - Lists in a list will output results by grouped SOC for SOCs in the sub-list\n",
    "    # - To \"select all\" (i.e. not filter by SOC and generate a single coefficient) remove from dict or input [[1, 2, 3, 4]]\n",
    "purposes_with_soc_selection = {\n",
    "    \"Commute\": [[1, 2, 3, 4]],\n",
    "    \"Employer's Business\": [[1, 2, 3, 4]]\n",
    "}\n",
    " \n",
    "# Select education parameters\n",
    "edu_aggregation = False # Can be False (as many coeffs as inputs) OR True (1 coeff) - Recommended to leave as False to avoid summing employment to students. Setting to True also overwrites pupil_aggregation\n",
    "pupil_aggregation = False # Can be False (as many coeffs as levels of edu in pupil data) OR True (1 coeff from pupil data). Overwritten if edu_aggregation == True\n",
    "sic_education = [] # Can be empty list [] for no SIC data OR a list of 1 or more 2 digit SIC codes (as int) for SIC data\n",
    "pupil_education = {'pupil': 'Keep',\n",
    "                   'higher': 'Drop',\n",
    "                   'further': 'Keep'} # 'Keep' to use level of edu from pupil data, 'Drop' to exclude it (all set to 'Drop' permitted)\n",
    " \n",
    "# Create education correspondence to pass to purpose list\n",
    "education_correspondence = sic_education.copy()\n",
    "for level, status in pupil_education.items():\n",
    "    if status == 'Keep':\n",
    "        education_correspondence.append(str(level))\n",
    "\n",
    "# Set Visit Friend parameters - the type of visit friend\n",
    "visit_friend_data = {'households': 'Keep',\n",
    "                     'population': 'Drop'} # 'Keep' to use these data in Visit Friend, 'Drop' to exclude it. Expected to be one or the other, not both\n",
    " \n",
    "visit_friend_correspondence = []\n",
    "for data, status in visit_friend_data.items():\n",
    "    if status == 'Keep':\n",
    "        visit_friend_correspondence.append(str(data))\n",
    "\n",
    "# Set the purpose correspondence\n",
    "    # If a purpose is turned \"On\" in purpose_inclusion, it MUST be in this list\n",
    "    # If a purpose is turned \"Off\" in purpose_inclusion, any value corresponding to it in this dictonary will be ignored\n",
    "    # \"All_SICs\" for all SIC purposes\n",
    "    # [int, ..., int] (list of ints corresponding to SIC 2 digit codes) to select those 2 digit codes in SIC data\n",
    "        # Each individual int here creates a set of coefficients for each group of Area Types\n",
    "        # Sub lists of ints laso allowed here - these will aggregate SICs within the sublist to a single coefficient per AT group\n",
    "    # \"Education\" to use the education selection above\n",
    "    # \"Visit Friend\" TODO\n",
    "correspondence_list_purpose = {\n",
    "    \"Commute\": \"All_SICs\",\n",
    "    \"Employer's Business\": \"All_SICs\",\n",
    "    \"Education\": education_correspondence,\n",
    "    \"Shopping\": [45, 47], \n",
    "    \"Personal Business\": [75, 77, 86, 96],\n",
    "    \"Social\": [56, 91, 93],\n",
    "    \"Visit Friend\": visit_friend_correspondence,\n",
    "    \"Holiday\": [55, 91]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ed997",
   "metadata": {},
   "source": [
    "# Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b78664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths, file names and working directory\n",
    "rootdir = 'I:/NTS/NorMITs WP1/Built Attractions'\n",
    "inputdirname = 'Input'\n",
    "edudirname = 'Data/Input'\n",
    "ntsdirname = 'PythonTesting'\n",
    "\n",
    "inputdirpath = os.path.join(rootdir, inputdirname)\n",
    "edudirpath = os.path.join(rootdir, edudirname)\n",
    "ntsdirpath = os.path.join(rootdir, ntsdirname)\n",
    "modeldirpath = os.path.join(rootdir, modeldirname)\n",
    "\n",
    "if not os.path.exists(modeldirpath):\n",
    "    os.makedirs(modeldirpath)\n",
    "\n",
    "lsoatomsoaname = 'LSOA21toMSOA 2011Lookup.csv' # Generated by AtkinsRealis from the shape file shared by TfN in Y:\\Data Strategy\\GIS Shapefiles\\NorMITs 2024 zone system\\GB LSOA2021 and DZ2011 Clipped\n",
    "lsoa2011tomsoaname = 'msoa_to_lsoa_correspondence.csv'\n",
    "msoatoatname = 'uk_msoa_iz_2011_area_types.csv' # From TfN\n",
    "msoatouaname = 'msoa_ua1998_lookup.csv' # From TfN\n",
    "ntsname = 'NTS_Data_Processed_v0.5.csv' # Processed by AtkinsRealis\n",
    "socsicpath = 'I:/NorMITs Land Use/import/SOC mix/soc_2_digit_sic_2018.csv' # From TfN\n",
    "hhpoppath = 'I:/NorMITs Land Use/base_land_use/iter4k/01 Process/3.2.4_land_use_formatting/resi_property_msoa_agg_prt_2018_dwells+pop.csv' # From TfN\n",
    "msoatogorpath = 'I:/Data/Zone Translations/msoa_to_gor_correspondence.csv' # From TfN\n",
    "\n",
    "# Education paths - All as provided by TfN\n",
    "eduengpupilpath = 'spc_school_level_underlying_data_lsoa21_202223.csv'# Uses 2021 LSOA\n",
    "eduscopupilpath = 'pupils22_scotland_dz2011.csv'# Uses 2021 LSOA\n",
    "eduwalpupilpath = 'wales_lsoa-floorspace_pupils22.csv'# Uses 2021 LSOA\n",
    "eduengfepath = 'further_education_england_202223.csv' # Uses 2011 (but labelled as 2021) LSOA\n",
    "eduscofepath = 'further_education_scotland_202122.csv'# Uses 2011 (but labelled as 2021) LSOA\n",
    "eduwalfepath = 'further_education_wales_202122.csv'# Uses 2021 LSOA - does not match other FE LSOA years. Has issues with data back to TfNs source\n",
    "edugbhepath = 'higher_education_202122.csv' # Uses 2021 LSOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45763879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data - lookups\n",
    "df_msoaTolsoa = pd.read_csv(os.path.join(edudirpath, lsoatomsoaname)) # The 2021 LSOA to 2011 MSOA correspondence is in here\n",
    "df_lsoa2011Tomsoa = pd.read_csv(os.path.join(edudirpath, lsoa2011tomsoaname)) # Required as Further Education (England and Scotland) is by 2011 LSOA\n",
    "df_msoatoatlookup_in = pd.read_csv(os.path.join(inputdirpath, msoatoatname))\n",
    "df_msoaUAlookup = pd.read_csv(os.path.join(inputdirpath, msoatouaname))\n",
    "df_msoatogorlookup = pd.read_csv(msoatogorpath)\n",
    "\n",
    "# NTS Data\n",
    "df_nts = pd.read_csv(os.path.join(ntsdirpath, ntsname)).drop(['Unnamed: 0'], axis=1) # Drop old index column\n",
    "\n",
    "# Employment Data\n",
    "df_socsic = pd.read_csv(socsicpath)\n",
    "\n",
    "# Visit Friends Data\n",
    "df_hhpop = pd.read_csv(hhpoppath)\n",
    "\n",
    "# Education Data\n",
    "df_pupilEngland = pd.read_csv(os.path.join(edudirpath, eduengpupilpath))\n",
    "df_pupilScotland = pd.read_csv(os.path.join(edudirpath, eduscopupilpath))\n",
    "df_pupilWales = pd.read_csv(os.path.join(edudirpath, eduwalpupilpath))\n",
    "df_furthereduEngland = pd.read_csv(os.path.join(edudirpath, eduengfepath))\n",
    "df_furthereduScotland = pd.read_csv(os.path.join(edudirpath, eduscofepath))\n",
    "df_furthereduWales = pd.read_csv(os.path.join(edudirpath, eduwalfepath))\n",
    "df_higheredugb = pd.read_csv(os.path.join(edudirpath, edugbhepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8349b",
   "metadata": {},
   "source": [
    "# Inital checks and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e838e3b",
   "metadata": {},
   "source": [
    "## Check Area Type Grouping by Purpose and Main Purpose selection match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83321d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort out purpose selection\n",
    "purposesUsed = []\n",
    "for key, value in purpose_inclusion.items():\n",
    "    if value != \"On\":\n",
    "        # Remove purposes that are not \"On\" from the dictonary that sets the processing\n",
    "        del correspondence_list_purpose[key]\n",
    "        purposesUsed.append(key)\n",
    "\n",
    "# Check purpose to AT grouping and flag issues\n",
    "checkedPurposesByATgroup = []\n",
    "for grouping in at_purp_classifcation:\n",
    "    for purpose in at_purp_classifcation[grouping]:\n",
    "        if purpose not in checkedPurposesByATgroup:\n",
    "            checkedPurposesByATgroup.append(purpose)\n",
    "        else:\n",
    "            print(f'WARNING! - Purpose \"{purpose}\" appears twice in the purpose to Area Type grouping allocation')\n",
    "            print(f'This will generate multiple outputs for the {purpose} purpose')\n",
    "for purposeU in purposesUsed:\n",
    "    if purposeU not in checkedPurposesByATgroup:\n",
    "        print(f'WARNING! - Purpose \"{purpose}\" is not assigned an Area Type grouping, but is supposedly active')\n",
    "        print(f'As the Area Type grouping to which to assign the {purpose} purpose is unknown, it will be dropped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec17f3",
   "metadata": {},
   "source": [
    "## Initial data processing, independent of Area Type Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44ed95",
   "metadata": {},
   "source": [
    "### Lookup processing that is independent of Area Type Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cadc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msoaUAlookup = df_msoaUAlookup[['msoa_zone_id','ua_1998_zone_id']]\n",
    "df_msoaTolsoa = df_msoaTolsoa[['msoa11cd', 'lsoa21cd']]\n",
    "df_msoaTolsoa['msoa11cd'] = df_msoaTolsoa['msoa11cd'].str.split('/').str[0] # As some MSOAs in the data have a '/' in them seperating two options for a single LSOA (2021 LSOA correspondence only)\n",
    "df_msoaTolsoa = df_msoaTolsoa.rename(columns={'lsoa21cd': 'lsoa'})\n",
    "df_lsoa2011Tomsoa = df_lsoa2011Tomsoa[['lsoa_zone_id', 'msoa_zone_id']]\n",
    "df_lsoa2011Tomsoa = df_lsoa2011Tomsoa.rename(columns={'lsoa_zone_id': 'lsoa', 'msoa_zone_id': 'msoa11cd'})\n",
    "\n",
    "# Create UA to GOR lookup - can be dropped if the inital NTS processing is made part of this workbook\n",
    "# Used to reattach GOR to output dfs without risk of odd things happening to it in groupbys\n",
    "df_ua_to_gor = df_msoatogorlookup.merge(\n",
    "    df_msoaUAlookup[['msoa_zone_id', 'ua_1998_zone_id']],\n",
    "    on='msoa_zone_id',\n",
    "    how='outer'\n",
    ")[['ua_1998_zone_id', 'gor']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Do the preliminary processing on the Visit Friends dataset\n",
    "df_hhpop_trips = df_hhpop.rename(columns={'ZoneID':'msoa_zone_id'})\n",
    "df_hhpop_trips['households'] = df_hhpop_trips['population'] / df_hhpop_trips['household_occupancy_18']\n",
    "df_hhpop_trips = df_hhpop_trips.groupby(['msoa_zone_id'])[['population', 'households']].sum().reset_index()\n",
    "df_hhpop_trips = df_hhpop_trips.merge(df_msoaUAlookup, on='msoa_zone_id', how='inner')\n",
    "df_hhpop_trips = df_hhpop_trips.rename(columns={'msoa_zone_id': 'msoa11cd'})\n",
    "# df_hhpop_trips = df_hhpop_trips.groupby(['ua_1998_zone_id'])[['population', 'number_of_households']].sum().reset_index()\n",
    "\n",
    "# Rename the msoa_zone_id column now we have used it in the GOR lookup\n",
    "df_msoaUAlookup_ren = df_msoaUAlookup.rename(columns={'msoa_zone_id': 'msoa11cd'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417702d5",
   "metadata": {},
   "source": [
    "### NTS data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c01f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for renaming NTS purpose codes\n",
    "correspondence_list_purpose_name = {\n",
    "    1: \"Commute\",\n",
    "    2: \"Employer's Business\",\n",
    "    3: \"Education\",\n",
    "    4: \"Shopping\",\n",
    "    5: \"Personal Business\",\n",
    "    6: \"Social\",\n",
    "    7: \"Visit Friend\",\n",
    "    8: \"Holiday\"\n",
    "}\n",
    "\n",
    "# Reverse correspondence for final coefficient output\n",
    "inv_correspondence_list_purpose_name = {v: k for k, v in correspondence_list_purpose_name.items()}\n",
    "\n",
    "# Get purpose (and SOC where valid) together as a column\n",
    "df_nts['purpose_name'] = df_nts['purpose'].map(correspondence_list_purpose_name)\n",
    "\n",
    "# Create purpose_SOC column for purposes that may require it\n",
    "df_nts['purpose_SOC'] = np.where(df_nts['purpose_name'].isin(purposes_with_soc_selection),\n",
    "                                 df_nts['purpose_name'].astype(str) + df_nts['soc'].astype(str),\n",
    "                                 df_nts['purpose_name'].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5087526f",
   "metadata": {},
   "source": [
    "### Education data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a986cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def education_lsoa_msoa_join(education_df, lsoayear):\n",
    "    if lsoayear == 2011:\n",
    "        education_df = education_df.merge(df_lsoa2011Tomsoa, on='lsoa', how='inner')\n",
    "    else:\n",
    "        if  lsoayear != 2021:\n",
    "            print(f'WARNING! - Only 2011 and 2021 are valid lsoa years here, not: {str(lsoayear)}')\n",
    "            print('Defaulting to 2021 - CHECK YOUR OUTPUTS AS ROWS ARE LIKELY TO HAVE BEEN DROPPED UNEXPECTEDLY')\n",
    "        education_df = education_df.merge(df_msoaTolsoa, on='lsoa', how='inner')\n",
    "    education_df = education_df.drop(columns=['lsoa'], axis=1)\n",
    "    education_df = education_df.groupby(['msoa11cd']).sum()\n",
    "    \n",
    "    return education_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in input data to get a consistent standard and create the GB dataframes (where required)\n",
    "\n",
    "# As different levels of education by geography use different LSOA numbering systems, this is required\n",
    "edu_lsoa_year_lookup = {\n",
    "    'Pupils England': 2021,\n",
    "    'Pupils Scotland': 2021,\n",
    "    'Pupils Wales': 2021,\n",
    "    'Further England': 2011,\n",
    "    'Further Scotland': 2011,\n",
    "    'Further Wales': 2021, # Odd one out in further education. Does not appear to have been processed correctly prior to input here as of 21/05/2024\n",
    "    'Higher GB': 2021\n",
    "}\n",
    "\n",
    "df_pupilEngland = df_pupilEngland.rename(columns={'lsoa21cd': 'lsoa', 'fte pupils': 'pupils'})\n",
    "df_pupilWales = df_pupilWales.rename(columns={'lsoa21cd': 'lsoa'})\n",
    "df_pupilEngland = education_lsoa_msoa_join(df_pupilEngland, edu_lsoa_year_lookup['Pupils England'])\n",
    "df_pupilScotland = education_lsoa_msoa_join(df_pupilScotland, edu_lsoa_year_lookup['Pupils Scotland'])\n",
    "df_pupilWales = education_lsoa_msoa_join(df_pupilWales, edu_lsoa_year_lookup['Pupils Wales'])\n",
    "df_pupilGB = pd.concat([df_pupilEngland, df_pupilScotland, df_pupilWales], axis=0)\n",
    "df_pupilGB = df_pupilGB.rename(columns={'pupils': 'student_count'})\n",
    "\n",
    "df_furthereduEngland = df_furthereduEngland.rename(columns={'lsoa21cd': 'lsoa'})\n",
    "df_furthereduWales = df_furthereduWales.rename(columns={'lsoa21cd': 'lsoa', 'Sum of dist_fe': 'fe_students'})\n",
    "df_furthereduEngland = education_lsoa_msoa_join(df_furthereduEngland, edu_lsoa_year_lookup['Further England'])\n",
    "df_furthereduScotland = education_lsoa_msoa_join(df_furthereduScotland, edu_lsoa_year_lookup['Further Scotland'])\n",
    "df_furthereduWales = education_lsoa_msoa_join(df_furthereduWales, edu_lsoa_year_lookup['Further Wales'])\n",
    "df_feGB = pd.concat([df_furthereduEngland, df_furthereduWales, df_furthereduScotland], axis=0)\n",
    "df_feGB = df_feGB.rename(columns={'fe_students': 'student_count'})\n",
    "\n",
    "df_highereduGB = df_higheredugb.rename(columns={'lsoa21cd': 'lsoa', 'he_students': 'student_count'})\n",
    "df_highereduGB = education_lsoa_msoa_join(df_highereduGB, edu_lsoa_year_lookup['Higher GB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102bfa5",
   "metadata": {},
   "source": [
    "# Main Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e980d",
   "metadata": {},
   "source": [
    "## Functions called in Main Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4397008",
   "metadata": {},
   "source": [
    "### Select Area Type Grouping correspondance table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8aa9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AT_grouping_table(atGroupingName):\n",
    "        if atGroupingName == 'grouped_v1':\n",
    "            corr_list_at = {\n",
    "                1: 'Group_1-8',\n",
    "                2: 'Group_1-8',\n",
    "                3: 'Group_1-8',\n",
    "                4: 'Group_1-8',\n",
    "                5: 'Group_1-8',\n",
    "                6: 'Group_1-8',\n",
    "                7: 'Group_1-8',\n",
    "                8: 'Group_1-8',\n",
    "                9: 'Group_9-15',\n",
    "                10: 'Group_9-15',\n",
    "                11: 'Group_9-15',\n",
    "                12: 'Group_9-15',\n",
    "                13: 'Group_9-15',\n",
    "                14: 'Group_9-15',\n",
    "                15: 'Group_9-15',\n",
    "                16: 'Group_16-17',\n",
    "                17: 'Group_16-17',\n",
    "                18: 'Group_18_19',\n",
    "                19: 'Group_18_19',\n",
    "                20: 'Group_20'\n",
    "            }\n",
    "        elif atGroupingName == 'grouped_v2':\n",
    "            corr_list_at = {\n",
    "                1: 'Group_1,3,16',\n",
    "                2: 'Group_2,6,8,17,19,20',\n",
    "                3: 'Group_1,3,16',\n",
    "                4: 'Group_4,5,7,14,15',\n",
    "                5: 'Group_4,5,7,14,15',\n",
    "                6: 'Group_2,6,8,17,19,20',\n",
    "                7: 'Group_4,5,7,14,15',\n",
    "                8: 'Group_2,6,8,17,19,20',\n",
    "                9: 'Group_9,10,11,12,13',\n",
    "                10: 'Group_9,10,11,12,13',\n",
    "                11: 'Group_9,10,11,12,13',\n",
    "                12: 'Group_9,10,11,12,13',\n",
    "                13: 'Group_9,10,11,12,13',\n",
    "                14: 'Group_4,5,7,14,15',\n",
    "                15: 'Group_4,5,7,14,15',\n",
    "                16: 'Group_1,3,16',\n",
    "                17: 'Group_2,6,8,17,19,20',\n",
    "                18: 'Group_18',\n",
    "                19: 'Group_2,6,8,17,19,20',\n",
    "                20: 'Group_2,6,8,17,19,20'\n",
    "            }\n",
    "        elif atGroupingName == 'grouped_v3':\n",
    "            corr_list_at = {\n",
    "                1: 'Group_1,3,16',\n",
    "                2: 'Group_2,6,8,17,18,19,20',\n",
    "                3: 'Group_1,3,16',\n",
    "                4: 'Group_4,5,7,14,15',\n",
    "                5: 'Group_4,5,7,14,15',\n",
    "                6: 'Group_2,6,8,17,18,19,20',\n",
    "                7: 'Group_4,5,7,14,15',\n",
    "                8: 'Group_2,6,8,17,18,19,20',\n",
    "                9: 'Group_9,10,11,12,13',\n",
    "                10: 'Group_9,10,11,12,13',\n",
    "                11: 'Group_9,10,11,12,13',\n",
    "                12: 'Group_9,10,11,12,13',\n",
    "                13: 'Group_9,10,11,12,13',\n",
    "                14: 'Group_4,5,7,14,15',\n",
    "                15: 'Group_4,5,7,14,15',\n",
    "                16: 'Group_1,3,16',\n",
    "                17: 'Group_2,6,8,17,18,19,20',\n",
    "                18: 'Group_2,6,8,17,18,19,20',\n",
    "                19: 'Group_2,6,8,17,18,19,20',\n",
    "                20: 'Group_2,6,8,17,18,19,20'\n",
    "            }\n",
    "        elif atGroupingName == 'grouped_v4':\n",
    "            corr_list_at = {\n",
    "                       1: 'Group_1,16', \n",
    "                       2: 'Group_2,7,13,15,18',\n",
    "                       3: 'Group_3,8,9,10,12',\n",
    "                       4: 'Group_4,5,6,11,14',\n",
    "                       5: 'Group_4,5,6,11,14',\n",
    "                       6: 'Group_4,5,6,11,14',\n",
    "                       7: 'Group_2,7,13,15,18',\n",
    "                       8: 'Group_3,8,9,10,12',\n",
    "                       9: 'Group_3,8,9,10,12',\n",
    "                       10: 'Group_3,8,9,10,12',\n",
    "                       11: 'Group_4,5,6,11,14',\n",
    "                       12: 'Group_3,8,9,10,12',\n",
    "                       13: 'Group_2,7,13,15,18',\n",
    "                       14: 'Group_4,5,6,11,14',\n",
    "                       15: 'Group_2,7,13,15,18',\n",
    "                       16: 'Group_1,16',\n",
    "                       17: 'Group_17,19,20',\n",
    "                       18: 'Group_2,7,13,15,18',\n",
    "                       19: 'Group_17,19,20',\n",
    "                       20: 'Group_17,19,20'\n",
    "            }\n",
    "        elif atGroupingName == 'grouped_v5':\n",
    "            corr_list_at = {\n",
    "                       1: 'Group_1,5,20,9,12', \n",
    "                       2: 'Group_14,15,17,8,2,3,4,10,6,7', \n",
    "                       3: 'Group_14,15,17,8,2,3,4,10,6,7', \n",
    "                       4: 'Group_14,15,17,8,2,3,4,10,6,7', \n",
    "                       5: 'Group_1,5,20,9,12', \n",
    "                       6: 'Group_14,15,17,8,2,3,4,10,6,7', \n",
    "                       7: 'Group_14,15,17,8,2,3,4,10,6,7', \n",
    "                       8: 'Group_14,15,17,8,2,3,4,10,6,7', \n",
    "                       9: 'Group_1,5,20,9,12', \n",
    "                       10: 'Group_14,15,17,8,2,3,4,10,6,7', \n",
    "                       11: 'Group_13,11,18', \n",
    "                       12: 'Group_1,5,20,9,12', \n",
    "                       13: 'Group_13,11,18', \n",
    "                       14: 'Group_14,15,17,8,2,3,4,10,6,7', \n",
    "                       15: 'Group_14,15,17,8,2,3,4,10,6,7', \n",
    "                       16: 'Group_16', \n",
    "                       17: 'Group_14,15,17,8,2,3,4,10,6,7', \n",
    "                       18: 'Group_13,11,18', \n",
    "                       19: 'Group_19', \n",
    "                       20: 'Group_1,5,20,9,12'\n",
    "            }\n",
    "        elif atGroupingName == 'individual':\n",
    "            corr_list_at = {\n",
    "                1: 'Group_1',\n",
    "                2: 'Group_2',\n",
    "                3: 'Group_3',\n",
    "                4: 'Group_4',\n",
    "                5: 'Group_5',\n",
    "                6: 'Group_6',\n",
    "                7: 'Group_7',\n",
    "                8: 'Group_8',\n",
    "                9: 'Group_9',\n",
    "                10: 'Group_10',\n",
    "                11: 'Group_11',\n",
    "                12: 'Group_12',\n",
    "                13: 'Group_13',\n",
    "                14: 'Group_14',\n",
    "                15: 'Group_15',\n",
    "                16: 'Group_16',\n",
    "                17: 'Group_17',\n",
    "                18: 'Group_18',\n",
    "                19: 'Group_19',\n",
    "                20: 'Group_20'\n",
    "            }\n",
    "        else:\n",
    "            if atGroupingName != 'none':\n",
    "                print(f'WARNING! - Unknown at_classification: {str(atGroupingName)}')\n",
    "                print(\"Defaulting to to AreaType Grouping 'None'\")\n",
    "            corr_list_at = {\n",
    "                1: 'Group_All',\n",
    "                2: 'Group_All',\n",
    "                3: 'Group_All',\n",
    "                4: 'Group_All',\n",
    "                5: 'Group_All',\n",
    "                6: 'Group_All',\n",
    "                7: 'Group_All',\n",
    "                8: 'Group_All',\n",
    "                9: 'Group_All',\n",
    "                10: 'Group_All',\n",
    "                11: 'Group_All',\n",
    "                12: 'Group_All',\n",
    "                13: 'Group_All',\n",
    "                14: 'Group_All',\n",
    "                15: 'Group_All',\n",
    "                16: 'Group_All',\n",
    "                17: 'Group_All',\n",
    "                18: 'Group_All',\n",
    "                19: 'Group_All',\n",
    "                20: 'Group_All'\n",
    "            }\n",
    "        return corr_list_at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b58c8",
   "metadata": {},
   "source": [
    "### Employment data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sub functions used in the all_purposes_sics function\n",
    "\n",
    "def get_sic_formatted(df_sic, sic_val):\n",
    "    # Doing the grouping and column renaming of the SIC table\n",
    "    df_sic = df_sic.groupby(['ua_1998_zone_id', 'ATgroup']).sum('seg_jobs').reset_index()\n",
    "    df_sic = df_sic[['ua_1998_zone_id', 'ATgroup', 'seg_jobs']]\n",
    "    df_sic = df_sic.pivot_table(index=['ua_1998_zone_id'], columns=['ATgroup'],\n",
    "                                values='seg_jobs', aggfunc='sum').reset_index()\n",
    "    df_sic.fillna(0, inplace=True)\n",
    "    # Loop through each column in the DataFrames\n",
    "    for col in df_sic.columns:\n",
    "        # Check if it is the column to be excluded\n",
    "        if col != 'ua_1998_zone_id':\n",
    "            # Prefix the string to the column name\n",
    "            df_sic.rename(columns={col: sic_val + col}, inplace=True)\n",
    "    \n",
    "    return df_sic\n",
    "\n",
    "def get_nts_formatted(df_nts_filtered, purpose_name, col_search):\n",
    "    # No SIC data in the NTS table, so no need to filter for that\n",
    "    # Does use SOC, so needs passing the SOC filtered table\n",
    "    # Also uses purpose, so needs passing purpose (incl. SOC when SOC is in)\n",
    "    df_nts_filtered = df_nts_filtered[df_nts_filtered['direction'] == nts_direction]\n",
    "    if type(purpose_name) is list: # A case when SOC filtering is occuring\n",
    "        df_nts_filtered = df_nts_filtered[df_nts_filtered[col_search].isin(purpose_name)]\n",
    "    else:\n",
    "        df_nts_filtered = df_nts_filtered[df_nts_filtered[col_search] == purpose_name]\n",
    "    df_nts_filtered = df_nts_filtered.groupby(['ua_1998_zone_id'])['expanded_trips'].sum().reset_index()\n",
    "    df_nts_filtered['ua_1998_zone_id'] = df_nts_filtered['ua_1998_zone_id'].astype(int)\n",
    "    \n",
    "    return df_nts_filtered\n",
    "    \n",
    "def merge_dataframes(df_list, column_name):\n",
    "    merged_df = df_list[0] # Get the first DataFrame in the list\n",
    "    # Merge all other DataFrames in the list based on the column\n",
    "    for i in range(1, len(df_list)):\n",
    "        merged_df = pd.merge(merged_df, df_list[i], on=column_name)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def sic_processing(df_sic_p, sic_selection):\n",
    "    if sic_selection == 'All':\n",
    "        val = 'AllSICs_'\n",
    "        sic_output_data = get_sic_formatted(df_sic_p, val) # Process SIC data\n",
    "    else:\n",
    "        dfs = []    \n",
    "        for value in sic_selection:\n",
    "            if type(value) is list:\n",
    "                df_sicx_p = df_sic_p[df_sic_p['sic_2d'].isin(value)] # Filter to a list of SICs to be grouped\n",
    "                val = ''.join(['SICs', '+'.join(value), '_'])\n",
    "            else:\n",
    "                df_sicx_p = df_sic_p[df_sic_p['sic_2d'] == value] # Filter to each SIC indivdually\n",
    "                val = ''.join(['SIC', str(value), '_'])\n",
    "            df_sicx_p = get_sic_formatted(df_sicx_p, val) # Process SIC data\n",
    "            dfs.append(df_sicx_p)\n",
    "        sic_output_data = merge_dataframes(dfs, 'ua_1998_zone_id') # Merge individual SIC dfs back together by purpose\n",
    "            \n",
    "    return sic_output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f8f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_purpose_sics(purpose, purposesoc, sic, df_socsicx):\n",
    "    if type(purposesoc) is list: # Will only occur when disagg of SOC is true for disagrable purposes\n",
    "        socs = []\n",
    "        for purpsoc in purposesoc:\n",
    "            socs.append(int(purpsoc[-1]))\n",
    "        df_sic_px =  df_socsicx[df_socsicx['soc_class'].isin(socs)]\n",
    "        df_nts_p = get_nts_formatted(df_nts, purposesoc, 'purpose_SOC') # Process NTS data (excludes SIC, so can be done now)\n",
    "        df_sic_processed = sic_processing(df_sic_px, sic)\n",
    "        df_final = df_nts_p.merge(df_sic_processed, on='ua_1998_zone_id', how='left')\n",
    "    \n",
    "    else:\n",
    "        df_nts_p = get_nts_formatted(df_nts, purpose, 'purpose_name') # Process NTS data (excludes SIC, so can be done now)\n",
    "        df_sic_processed = sic_processing(df_socsicx, sic)\n",
    "        df_final = df_nts_p.merge(df_sic_processed, on='ua_1998_zone_id', how='left')\n",
    "        \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660c7bc",
   "metadata": {},
   "source": [
    "### Education data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a function to process each level of education to UA level with applied TfN AT grouping\n",
    "def process_edu_by_level(df_edulevelGB, colprefix):\n",
    "    # Merge additional geographies onto df\n",
    "    # NOTE: These merges are NOT lossless. Possibility that msoa/lsoa codes not consisently from the same year\n",
    "#     df_edulevelGB = df_edulevelGB.merge(df_msoaTolsoa, on='lsoa', how='left') # Moved this line to initial processing\n",
    "    df_edulevelGB = df_edulevelGB.merge(df_msoaUAlookup_ren, on='msoa11cd', how='left')\n",
    "    df_edulevelGB = df_edulevelGB.merge(df_msoatoatlookup, on='msoa11cd', how='left')\n",
    "    df_edulevelGB.drop(columns=['msoa11cd'], inplace=True)\n",
    "    \n",
    "    # Reshape data to required format\n",
    "    df_edulevelGB = df_edulevelGB.groupby(['ua_1998_zone_id', 'ATgroup']).sum('student_count').reset_index()\n",
    "    df_edulevelGB = df_edulevelGB.pivot_table(index=['ua_1998_zone_id'], columns=['ATgroup'],\n",
    "                                              values='student_count', aggfunc='sum').reset_index()\n",
    "    df_edulevelGB = df_edulevelGB.rename_axis(None, axis=1)\n",
    "    \n",
    "    # Label columns as required\n",
    "    columns_to_prefix = [col for col in df_edulevelGB.columns if col != 'ua_1998_zone_id']\n",
    "    prefixed_columns = [f'{colprefix}_{col}' for col in columns_to_prefix]\n",
    "    new_columns = {old_col: new_col for old_col, new_col in zip(columns_to_prefix, prefixed_columns)}\n",
    "    df_edulevelGB = df_edulevelGB.rename(columns=new_columns)\n",
    "    df_edulevelGB.fillna(0, inplace=True)\n",
    "    df_edulevelGB['ua_1998_zone_id'] = df_edulevelGB['ua_1998_zone_id'].astype(int)\n",
    "    \n",
    "    # Drop UAs that are Scottish Islands. Seemingly missing from Employment Data, so make joins tricky\n",
    "    # Very remote areas (Shetland, Orkney and Western Isles), so unlikely to have a large impact on results\n",
    "    df_edulevelGB = df_edulevelGB.loc[~df_edulevelGB['ua_1998_zone_id'].isin([724, 727, 732])]\n",
    "    \n",
    "    df_edulevelGB = df_edulevelGB.set_index(['ua_1998_zone_id']) # required for concats/joining etc. after this function\n",
    "    \n",
    "    return df_edulevelGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95b9fe",
   "metadata": {},
   "source": [
    "### Regression modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51512eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressions(df_regression, tsoc, tpurp):\n",
    "    df_regression = df_regression.merge(df_ua_to_gor, on='ua_1998_zone_id', how='left')\n",
    "    columns_to_drop = ['expanded_trips', 'gor', 'ua_1998_zone_id']\n",
    "    X = df_regression.drop(columns=columns_to_drop, axis=1)\n",
    "    y = df_regression['expanded_trips'] # select the y column from the dataframe\n",
    "\n",
    "    # Instantiate the Ridge regression model with a specific alpha value\n",
    "    linear_regression = LinearRegression(positive = True, fit_intercept = False) # set the desired alpha value\n",
    "    # ridge_model = Ridge(alpha=1.0, fit_intercept=False, positive=True) # set the desired alpha value (currently 1.0, the default)\n",
    "    # Note that R model had alpha=0 which supposedly is the same as doing the basic \"linear regression\" model in Python\n",
    "\n",
    "    # Fit the model to the data\n",
    "    # ridge_model.fit(X, y)\n",
    "    # predicted_values = ridge_model.predict(X)\n",
    "    linear_regression.fit(X, y)\n",
    "    predicted_values = linear_regression.predict(X)\n",
    "    predicted_values\n",
    "    df_regression['predicted_values'] = predicted_values\n",
    "\n",
    "    # coefficients = ridge_model.coef_\n",
    "    coefficients = linear_regression.coef_\n",
    "    df_regression_variables = df_regression.drop(columns=columns_to_drop, axis=1)\n",
    "    coefficients_df = pd.DataFrame(\n",
    "        zip(df_regression_variables.columns, linear_regression.coef_),\n",
    "        columns=['feature', 'coefficient']\n",
    "    )\n",
    "    \n",
    "    X = df_regression[['expanded_trips']]\n",
    "    y = df_regression['predicted_values']\n",
    "\n",
    "    # Scatter plot\n",
    "    # plt.scatter(X, y)\n",
    "\n",
    "    # Fit the linear model\n",
    "    # model = Ridge(alpha=1.0, fit_intercept=False, positive=True)\n",
    "    model = LinearRegression(fit_intercept=False, positive=True)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Get the slope and r square\n",
    "    slope = model.coef_[0]\n",
    "    r_square = model.score(X, y)\n",
    "\n",
    "    # Plotting\n",
    "    plottitle = ''.join(['Model ', model_name, ' ', model_version, '; SOC: ', tsoc, '; Purpose: ', tpurp])\n",
    "    plotfname = plottitle.replace(';', '')\n",
    "    plotfname = plotfname.replace(':', '')\n",
    "    plotfname = plotfname.replace(' ', '_')\n",
    "    fig, ax = plt.subplots(figsize=(15,8))\n",
    "    sns.lineplot(x=df_regression['expanded_trips'], y=model.predict(X), ax=ax, color='black') # Plot the linear fit line\n",
    "    sns.scatterplot(data=df_regression, x='expanded_trips', y='predicted_values', hue='gor', ax=ax) # Plot data and colour by GOR\n",
    "    tl = ((ax.get_xlim()[1] - ax.get_xlim()[0])*0.010 + ax.get_xlim()[0],\n",
    "          (ax.get_ylim()[1] - ax.get_ylim()[0])*0.95 + ax.get_ylim()[0])\n",
    "    ax.text(tl[0], tl[1], r\"$R^2 = {}, Slope = {}$\".format(round(r_square, 2), round(slope, 2)), fontsize=14)\n",
    "    plt.legend(loc='right', bbox_to_anchor=(1.2, 0.5), ncol=1, title='GOR')\n",
    "    plt.title(plottitle, fontsize=17)\n",
    "    plt.savefig(os.path.join(modeldirpath, ''.join([plotfname, '.png'])))  # Save as PNG\n",
    "    # plt.savefig(os.path.join(modeldirpath, ''.join([plottitle, '.jpeg'])))  # Save as JPEG\n",
    "    plt.show()\n",
    "    \n",
    "    # Save out coefficient data\n",
    "    coeff_fname = '_'.join([model_name, model_version, tpurp, tsoc, 'coeffs.csv'])\n",
    "    coefficients_df.to_csv(os.path.join(modeldirpath, coeff_fname), index=False)\n",
    "    \n",
    "    return coefficients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b175ca",
   "metadata": {},
   "source": [
    "## Run Main Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a060e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_coeffs_list = []\n",
    "for at_classification, purposes in at_purp_classifcation.items():\n",
    "    correspondence_list_at = get_AT_grouping_table(at_classification)\n",
    "    \n",
    "    # Lookup tables dependent on Area Type grouping\n",
    "    df_msoatoatlookup = df_msoatoatlookup_in.copy()\n",
    "    df_msoatoatlookup['ATgroup'] = df_msoatoatlookup['tfn_at'].map(correspondence_list_at)\n",
    "    df_msoatoatlookup = df_msoatoatlookup[['msoa11cd', 'ATgroup']]\n",
    "\n",
    "    df_socsic_atua = df_msoatoatlookup.rename(columns={'msoa11cd': 'msoa_zone_id'})\n",
    "    df_socsic_atua = pd.merge(df_socsic, df_socsic_atua, on='msoa_zone_id', how='inner')\n",
    "    df_socsic_atua = pd.merge(df_socsic_atua, df_msoaUAlookup, on='msoa_zone_id', how='inner')\n",
    "    df_socsic_atua = df_socsic_atua[['sic_2d', 'soc_class','seg_jobs','ATgroup', 'ua_1998_zone_id']]\n",
    "    \n",
    "    # Loop through purposes and what they correpsond to\n",
    "    for p in purposes:\n",
    "        if purpose_inclusion[p] == 'On':\n",
    "            purpose_vals = correspondence_list_purpose[p]\n",
    "            if p == 'Visit Friend':\n",
    "                # Do Visit Friend - completely unrelated to employment, hence why it's done on its own\n",
    "                titlesoc = 'All'\n",
    "                \n",
    "                # Get NTS data NTS\n",
    "                df_for_reg = get_nts_formatted(df_nts, p, 'purpose_name')\n",
    "                \n",
    "                # Process hh/pop data\n",
    "                for vf_data in purpose_vals:\n",
    "                    # Attach TfN AT grouping to data\n",
    "                    df_vf_trips = df_hhpop_trips.merge(df_msoatoatlookup, on='msoa11cd', how='inner')\n",
    "\n",
    "                    # Pivoting\n",
    "                    df_vf_trips = df_vf_trips.groupby(['ua_1998_zone_id', 'ATgroup'])[[vf_data]].sum().reset_index()\n",
    "                    df_vf_trips = df_vf_trips.pivot_table(index=['ua_1998_zone_id'], columns=['ATgroup'],\n",
    "                                                values=vf_data, aggfunc='sum').reset_index()\n",
    "                    df_vf_trips.fillna(0, inplace=True)\n",
    "                    # Loop through each column in the DataFrames\n",
    "                    for col in df_vf_trips.columns:\n",
    "                        # Check if it is the column to be excluded\n",
    "                        if col != 'ua_1998_zone_id':\n",
    "                            # Prefix the string to the column name\n",
    "                            df_vf_trips.rename(columns={col: vf_data + '_' + col}, inplace=True)\n",
    "                    \n",
    "                    # Join to NTS\n",
    "                    df_for_reg = df_for_reg.merge(df_vf_trips, on='ua_1998_zone_id', how='left')\n",
    "                \n",
    "                # Regressions and outputs\n",
    "                print(p)\n",
    "                reg_inputs_fname = '_'.join([model_name, model_version, p, titlesoc, 'data.csv'])\n",
    "                df_for_reg.to_csv(os.path.join(modeldirpath, reg_inputs_fname), index=False)\n",
    "                p_coeffs = regressions(df_for_reg, titlesoc, p)\n",
    "                if titlesoc == 'All':\n",
    "                    titlesoc = '1 & 2 & 3 & 4' # Now naming is done, revert to all for coeff output\n",
    "                p_coeffs_list.append([p_coeffs, p, titlesoc])\n",
    "                    \n",
    "            else:\n",
    "                # First, get the purpose_vals sorted by what they apply to (employment data, education data, etc.)\n",
    "                edu_vals = []\n",
    "                sic_vals = []\n",
    "                if purpose_vals == 'All_SICs':\n",
    "                    sic_vals = 'All' # Force the list to become a string (len will still be > 0)\n",
    "                else:\n",
    "                    for v in purpose_vals:\n",
    "                        if v in range(1, 100):\n",
    "                            sic_vals.append(v)\n",
    "                        elif type(v) is list:\n",
    "                            if all(isinstance(x, int) for x in v) == True:\n",
    "                                   sic_vals.append(v)\n",
    "                        elif v in pupil_education:\n",
    "                            edu_vals.append(v)\n",
    "                        else:\n",
    "                            print(f'WARNING! - When processing purpose {p}, within the values, found unexpected input: {str(v)}')\n",
    "                            print(f'   - Did not process: {str(v)}')\n",
    "\n",
    "                # Now process data by purpose under the various selection conditions\n",
    "                if p in purposes_with_soc_selection:\n",
    "                    # Case where purpose is disagregated by SOC\n",
    "                    # Education data NOT processed here as it cannot be disaggregated by SOC \n",
    "                    for soc_val in purposes_with_soc_selection[p]:\n",
    "                        psocs = []\n",
    "                        if type(soc_val) is list:\n",
    "                            soc_val = sorted(soc_val)\n",
    "                            for socv in soc_val:\n",
    "                                psocs.append(''.join([p, str(socv)]))\n",
    "                            titlesoc = ' & '.join(map(str, soc_val))\n",
    "                            if titlesoc == '1 & 2 & 3 & 4':\n",
    "                                titlesoc = 'All' # Makes naming neater/clearer\n",
    "                        else:\n",
    "                            psocs.append(''.join([p, str(soc_val)]))\n",
    "                            titlesoc = str(soc_val)\n",
    "                        if len(sic_vals) > 0:\n",
    "                            df_for_reg = all_purpose_sics(p, psocs, sic_vals, df_socsic_atua)\n",
    "                        print(psocs)\n",
    "                        reg_inputs_fname = '_'.join([model_name, model_version, p, titlesoc, 'data.csv'])\n",
    "                        df_for_reg.to_csv(os.path.join(modeldirpath, reg_inputs_fname), index=False)\n",
    "                        p_coeffs = regressions(df_for_reg, titlesoc, p)\n",
    "                        if titlesoc == 'All':\n",
    "                            titlesoc = '1 & 2 & 3 & 4' # Now naming is done, revert to all for coeff output\n",
    "                        p_coeffs_list.append([p_coeffs, p, titlesoc])\n",
    "                else:\n",
    "                    reg_sel = 0\n",
    "                    if len(sic_vals) > 0:\n",
    "                        df_for_reg_sic = all_purpose_sics(p, p, sic_vals, df_socsic_atua)\n",
    "                        reg_sel = reg_sel + 1\n",
    "\n",
    "                    # Process education data\n",
    "                    if len(edu_vals) > 0:\n",
    "                        edu_dfs = []\n",
    "                        for level in edu_vals:\n",
    "                            puprefix = 'MainEduStudents'\n",
    "                            feprefix = 'FurtherEduStudents'\n",
    "                            heprefix = 'HigherEduStudents'\n",
    "                            if level == 'pupil':\n",
    "                                edu_dfs.append(process_edu_by_level(df_pupilGB, puprefix))\n",
    "                            elif level == 'further':\n",
    "                                edu_dfs.append(process_edu_by_level(df_feGB, feprefix))\n",
    "                            elif level == 'higher':\n",
    "                                edu_dfs.append(process_edu_by_level(df_highereduGB, heprefix))\n",
    "                            else:\n",
    "                                print(\n",
    "                                    f'WARNING! - Expected education level to be \"pupil\", \"further\" or \"higher\", but got {str(level)}'\n",
    "                                )\n",
    "                        reg_sel = reg_sel + 2\n",
    "                        # - TODO: look at options in input on how to combine levels of education\n",
    "                        if len(edu_dfs) > 1:\n",
    "                            if (pupil_aggregation == False) & (edu_aggregation == False):\n",
    "                                df_for_reg_edu = pd.concat(edu_dfs, axis=1)\n",
    "                                df_for_reg_edu = df_for_reg_edu.fillna(0)\n",
    "                            else:\n",
    "                                for edu_df in edu_dfs:\n",
    "                                    # Rename columns to remove prefixes\n",
    "                                    edu_df.columns = [col.lstrip(puprefix) for col in edu_df.columns]\n",
    "                                    edu_df.columns = [col.lstrip(feprefix) for col in edu_df.columns]\n",
    "                                    edu_df.columns = [col.lstrip(heprefix) for col in edu_df.columns]\n",
    "\n",
    "                                    # sum dfs on maximal (but ideally same) row count, same columns\n",
    "                                    df_for_reg_edu = pd.concat(\n",
    "                                        edu_dfs).fillna(0).groupby(['ua_1998_zone_id']).sum()\n",
    "\n",
    "                                    # Rename columns\n",
    "                                    columns_to_prefix = [col for col in df_for_reg_edu.columns if col != 'ua_1998_zone_id']\n",
    "                                    prefixed_columns = [f'All_Selected_Students_{col}' for col in columns_to_prefix]\n",
    "                                    new_columns = {\n",
    "                                        old_col: new_col for old_col, new_col in zip(columns_to_prefix, prefixed_columns)}\n",
    "                                    df_for_reg_edu = df_for_reg_edu.rename(columns=new_columns)      \n",
    "                        else:\n",
    "                            df_for_reg_edu = edu_dfs[0].copy()\n",
    "\n",
    "                    # Combine edu and sic dfs if both not empty here, otherwise take the non-empty df\n",
    "                    if reg_sel == 1:\n",
    "                        df_for_reg = df_for_reg_sic.copy()\n",
    "                    elif reg_sel == 2:\n",
    "                        df_for_reg = df_for_reg_edu.reset_index()\n",
    "                        # Get the NTS data in this case as the employment data will not pick it up for us\n",
    "                        df_nts_edu = get_nts_formatted(df_nts, p, 'purpose_name')\n",
    "                        df_for_reg = df_nts_edu.merge(df_for_reg, on='ua_1998_zone_id', how='left')\n",
    "                    elif reg_sel == 3:\n",
    "                        df_for_reg = df_for_reg_sic.set_index('ua_1998_zone_id')\n",
    "                        if edu_aggregation == False:\n",
    "                            # Append dfs to each other - maximal (but ideally same) row count, adding columns\n",
    "                            df_for_reg = df_for_reg_sic.merge(df_for_reg_edu, how='outer', on='ua_1998_zone_id')\n",
    "                            df_for_reg = df_for_reg.fillna(0)\n",
    "                        else:\n",
    "                            # Rename columns to remove prefixes\n",
    "                            df_for_reg_edu.columns = [col.lstrip(puprefix) for col in df_for_reg_edu.columns]\n",
    "                            df_for_reg_edu.columns = [col.lstrip(feprefix) for col in df_for_reg_edu.columns]\n",
    "                            df_for_reg_edu.columns = [col.lstrip(heprefix) for col in df_for_reg_edu.columns]\n",
    "                            df_for_reg_edu.columns = [col.lstrip('All_Selected_Students') for col in df_for_reg_edu.columns]\n",
    "                            df_for_reg.columns = ['_'.join(['Group', col.split('_')[-1]]) for col in df_for_reg.columns]\n",
    "                            df_for_reg.columns = [col.replace('Group_trips', 'expanded_trips') for col in df_for_reg.columns]\n",
    "\n",
    "                            # Split expanded trips off into its own df for merging\n",
    "                            df_exptrips = df_for_reg[['expanded_trips']]#.set_index('ua_1998_zone_id')\n",
    "                            df_for_reg.drop(columns=['expanded_trips'], axis=1, inplace=True)\n",
    "\n",
    "                            # sum dfs on maximal (but ideally same) row count, same column\n",
    "                            df_for_reg = pd.concat(\n",
    "                                [df_for_reg, df_for_reg_edu]).groupby(['ua_1998_zone_id']).sum()\n",
    "                            df_for_reg = pd.concat([df_exptrips, df_for_reg], axis=1)\n",
    "\n",
    "                            # Rename columns\n",
    "                            columns_to_prefix = [col for col in df_for_reg_edu.columns if col != 'ua_1998_zone_id']\n",
    "                            prefixed_columns = [f'All_Students_{col}' for col in columns_to_prefix]\n",
    "                            new_columns = {\n",
    "                                old_col: new_col for old_col, new_col in zip(columns_to_prefix, prefixed_columns)}\n",
    "                            df_for_reg_edu = df_for_reg_edu.rename(columns=new_columns) \n",
    "                    else:\n",
    "                        print('WARNING! - Unclear how to process merger of employment and education data')\n",
    "\n",
    "                    titlesoc = 'All'\n",
    "                    print(p)\n",
    "                    reg_inputs_fname = '_'.join([model_name, model_version, p, 'data.csv'])\n",
    "                    df_for_reg.to_csv(os.path.join(modeldirpath, reg_inputs_fname), index=False)\n",
    "                    p_coeffs = regressions(df_for_reg, titlesoc, p)\n",
    "                    if titlesoc == 'All':\n",
    "                        titlesoc = '1 & 2 & 3 & 4' # Now naming is done, revert to all for coeff output\n",
    "                    p_coeffs_list.append([p_coeffs, p, titlesoc])\n",
    "\n",
    "pcs = []\n",
    "for pcl in p_coeffs_list:\n",
    "    pc = pcl[0]\n",
    "    p = pcl[1]\n",
    "    s = pcl[2]\n",
    "    pc['purpose'] = p\n",
    "    pc['purpose'] = pc['purpose'].map(inv_correspondence_list_purpose_name).astype(int)\n",
    "    pc['feature'] = pc['feature'].str.replace('Group_', '')\n",
    "    pc.insert(len(pc.columns)-1, 'tfn_area_types', pc.pop('feature'))\n",
    "    pc['soc'] = s\n",
    "    pc['soc'] = pc['soc'].str.split(' & ')\n",
    "    pc = pc.explode('soc', ignore_index=True)\n",
    "    pc['soc'] = pc['soc'].astype(int)\n",
    "    pc['selection_sic_code_or_data'] = pc['tfn_area_types'].str.split('_').str[0]\n",
    "    pc['tfn_area_types'] = pc['tfn_area_types'].str.split('_').str[1]\n",
    "    pc['tfn_area_types'] = pc['tfn_area_types'].str.split(',')\n",
    "    pc = pc.explode('tfn_area_types', ignore_index=True)\n",
    "    pc['tfn_area_types'] = pc['tfn_area_types'].astype(int)\n",
    "    pc.insert(len(pc.columns)-1, 'coefficient', pc.pop('coefficient'))\n",
    "    pcs.append(pc)\n",
    "\n",
    "all_coeffs = pd.concat(pcs, axis=0, ignore_index=True)\n",
    "ac_fname = '_'.join([model_name, model_version, 'all_coeffs.csv'])\n",
    "all_coeffs.to_csv(os.path.join(modeldirpath, ac_fname), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba01731",
   "metadata": {},
   "source": [
    "## A note on scikit-learn and ridge regression\n",
    "\n",
    "To use the Ridge regression model with positive coefficients only, scikit-learn v1.0.0 (September 2021) or higher is required. The VMs used in the creation of this script do not have it installed and difficulties have been encounter attempting to upgrade this. The script has been trialled on a machine with a later version installed when running the ridge regression and this was successful. The cell below will list the scikit-learn version installed on the machine on which the script is currently running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2cb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
