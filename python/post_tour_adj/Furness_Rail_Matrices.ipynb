{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\G'\n",
      "C:\\Users\\Jimny\\AppData\\Local\\Temp\\ipykernel_9392\\256370786.py:1: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThis script builds on P:\\\\GBCBA\\\\HandT\\\\CQ\\\\ProjectsÅ’7104-NorMITs Demand 2024-ADDY4067  Technical\\x02 TourModel\\\\Develop Tour Model\\\\Rail Coverage Analysis\\x01_processing\\\\ProcessMatrix\\\\JoinMatrixToGeography_v0.2.ipynb\\nIt is designed to ultimately obtain a furnessed version of the rail output matrix based on the rail ticketing data proportions\\nVersioning to be handled by Git\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script builds on P:\\GBCBA\\HandT\\CQ\\Projects\\5227104-NorMITs Demand 2024-ADDY4067\\40 Technical\\02 TourModel\\Develop Tour Model\\Rail Coverage Analysis\\01_processing\\ProcessMatrix\\JoinMatrixToGeography_v0.2.ipynb\n",
    "It is designed to ultimately obtain a furnessed version of the rail output matrix based on the rail ticketing data proportions\n",
    "Versioning to be handled by Git\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "# TfN packages\n",
    "from caf.distribute import furness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jimny\\AppData\\Local\\Temp\\ipykernel_9392\\1579193846.py:18: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  odm_in_df = pd.read_csv(os.path.join(inputs_dir, odm_file))\n"
     ]
    }
   ],
   "source": [
    "# Set model version\n",
    "model_ver = 'v5'\n",
    "\n",
    "# Set directories\n",
    "inputs_dir = r'I:\\NTS\\imports\\tour_adjust_imports'\n",
    "msoa_dir = r'I:\\NTS\\imports'\n",
    "tour_model_dir = r'I:\\NTS\\outputs\\tour\\reports'\n",
    "\n",
    "# Set file names\n",
    "odm_file = 'ODM_for_rdm_2022-23.csv'\n",
    "msoa_county_file = 'msoa11cd_correspondence.csv'\n",
    "stn_geo_file = 'station_attributes_on_TfN_geography.csv'\n",
    "sector_file = 'bespoke_sectors_v1.1.csv'\n",
    "lrtu_file = 'lrt0101.csv'\n",
    "model_file = 'matrix_county_output.csv'\n",
    "\n",
    "# Import data\n",
    "odm_in_df = pd.read_csv(os.path.join(inputs_dir, odm_file))\n",
    "msoa_county_in_df = pd.read_csv(os.path.join(msoa_dir, msoa_county_file))\n",
    "stn_geo_in_df = pd.read_csv(os.path.join(inputs_dir, stn_geo_file))\n",
    "sector_in_df = pd.read_csv(os.path.join(inputs_dir, sector_file))\n",
    "lrtu_in_df = pd.read_csv(os.path.join(inputs_dir, lrtu_file), skiprows=7)\n",
    "lrtu_in_df.columns = lrtu_in_df.columns.str.split('[').str[0].str.strip() # Some processing required here to make column names tidier\n",
    "model_in_df = pd.read_csv(os.path.join(tour_model_dir, model_ver, model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other inputs\n",
    "Some manual inputs that set values later in the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set light rail inputs\n",
    "# Year for which to extract the Light Rail, Tramway and Underground data\n",
    "lrtu_year_in = 2023\n",
    "# Proportion of trips on the London Underground, London Trams and Docklands\n",
    "# Light Railway that are considered to be \"unique\" (i.e. not double counted\n",
    "# with another rail mode)\n",
    "lrtu_london_scale_in = 0.25\n",
    "# Proportion of trips on Light Rail, Tramway and Underground systems outside of\n",
    "# London that are considered to be \"unique\" (i.e. not double counted with\n",
    "# another rail mode)\n",
    "lrtu_nonlondon_scale_in = 0.5\n",
    "\n",
    "# For each Light Rail, Tramway or Underground system in GB,\n",
    "# set the sector in which it is located.\n",
    "# Done at sector level as some of these systems cross county borders\n",
    "lrtu_systems_in = {\n",
    "    'Docklands Light Railway': 'London',\n",
    "    'London Trams': 'London',\n",
    "    'Nottingham Express Transit': 'East Midlands North',\n",
    "    'West Midlands Metro': 'West Midlands South',\n",
    "    'Sheffield Supertram': 'South Yorkshire',\n",
    "    'Tyne and Wear Metro': 'Tyne and Wear',\n",
    "    'Manchester Metrolink': 'Greater Manchester',\n",
    "    'Blackpool Tramway': 'Lancashire',\n",
    "    'Edinburgh Trams': 'Scotland',\n",
    "    'London Underground': 'London',\n",
    "    'Glasgow Subway': 'Scotland'\n",
    "}\n",
    "\n",
    "# Set counties for stations that are located outside of the MSOA shapefile,\n",
    "# so get missed off the correspondence. This is a table here in case the station\n",
    "# shapefile is updated to add new stations\n",
    "\n",
    "# Need to account for:\n",
    "#  - Blackfriars (5112) - Bad join in the GIS as it's in the middle of the Thames\n",
    "#  - Portsmouth Harbour (5540) - Bad join in the GIS as it's in the harbour\n",
    "#  - Ryde Pier Head (5541) - Bad join as in the GIS as it's in the sea\n",
    "\n",
    "# Counties to allocate stations to:\n",
    "#  - Blackfriars -> Inner London (County 17)\n",
    "#  - Portsmouth Harbour -> Hampshire (County 35)\n",
    "#  - Ryde Pier Head -> Hampshire (County 35)\n",
    "\n",
    "stn_county_infill_df = pd.DataFrame(\n",
    "    columns=['National Location Code', 'county', 'county_nm'],\n",
    "    data=[[5112, 17, 'Inner London'],\n",
    "          [5540, 35, 'Hampshire'],\n",
    "          [5541, 35, 'Hampshire']]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to process rail ticketing/journey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lrtu_data(\n",
    "    lrtu_df, lrtu_year, lrtu_systems, lrtu_london_scale, lrtu_nonlondon_scale):\n",
    "    \"\"\"\n",
    "    Process light rail, tramway and underground data to get an annual journey\n",
    "    count (for \"unique\", i.e. not double counted with another rail mode) by\n",
    "    sector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lrtu_df: pandas df\n",
    "        Light Rail, Tramway and Underground annual journey data by system as\n",
    "        read in by this script\n",
    "    lrtu_year: int\n",
    "        Year for which to extract the Light Rail, Tramway and Underground data\n",
    "        It is the year in which the finacial year ends\n",
    "        It should match the year for which the national rail odm is downloaded\n",
    "    lrtu_systems: dict\n",
    "        Dictionary relating each Light Rail, Tramway or Underground system in\n",
    "        GB to the sector in which it is located\n",
    "    lrtu_london_scale: float\n",
    "        Expected range 0.0 to 1.0\n",
    "        Proportion of trips on the London Underground, London Trams and\n",
    "        Docklands Light Railway that are considered to be \"unique\" (i.e. not\n",
    "        double counted with another rail mode)\n",
    "    lrtu_nonlondon_scale: float\n",
    "        Expected range 0.0 to 1.0\n",
    "        Proportion of trips on Light Rail, Tramway and Underground systems\n",
    "        outside of London that are considered to be \"unique\" (i.e. not double\n",
    "        counted with another rail mode)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    lrtu_df: pandas df\n",
    "        For the selected year, the estimate of the number of \"unique\" (i.e. not\n",
    "        double counted with another rail mode) journeys by Light Rail, Tramway\n",
    "        and Underground for the sectors in which such systems are located.\n",
    "        This is an annual total\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic logic checks on inputs\n",
    "    yearnow = datetime.now().year\n",
    "    if (not 2013 < lrtu_year <= yearnow) or (type(lrtu_year) is not int):\n",
    "        print('WARNING: Unexpected input year for Light Rail, Tramway and Underground data')\n",
    "        print(f'Expected an interger year between 2014 and {yearnow}')\n",
    "        print(f'Instead, got {lrtu_year}')\n",
    "    if not 0 < lrtu_london_scale <= 1:\n",
    "        print('WARNING: London scaling factor expected to be greater than 0, less the or equal to 1')\n",
    "        print(f'Instead got London scaling factor of {lrtu_london_scale}')\n",
    "    if not 0 < lrtu_nonlondon_scale <= 1:\n",
    "        print('WARNING: Outside London scaling factor expected to be greater than 0, less the or equal to 1')\n",
    "        print(f'Instead got outside London scaling factor of {lrtu_nonlondon_scale}')\n",
    "    \n",
    "    # Process to account for odd formatting of source\n",
    "    lrtu_df = lrtu_df.dropna(axis=1, how='all')\n",
    "    lrtu_df = lrtu_df.dropna(axis=0, how='all')\n",
    "    lrtu_df = lrtu_df.rename(columns={'Financial year ending March': 'Year'})\n",
    "    lrtu_df['Year'] = lrtu_df['Year'].astype(int)\n",
    "\n",
    "    # Select data we are interested in and reformat to a system-based index\n",
    "    lrtu_df = lrtu_df.loc[lrtu_df['Year'] == lrtu_year]\n",
    "    lrtu_df = lrtu_df.set_index(['Year'])\n",
    "    lrtu_df = lrtu_df.transpose().reset_index()\n",
    "    lrtu_df = lrtu_df.rename_axis(None, axis=1)\n",
    "    lrtu_df = lrtu_df.rename(\n",
    "        columns={'index': 'System', lrtu_year: 'Yearly Journeys'})\n",
    "\n",
    "    # Convert yearly journeys to absolutes (and make sure they are numeric!)\n",
    "    # Note this bit will fall over if you pick a year before all systems\n",
    "    #   were returning data (i.e. some cells are '[w]')\n",
    "    lrtu_df['Yearly Journeys'] = lrtu_df['Yearly Journeys'].astype(str)\n",
    "    lrtu_df['Yearly Journeys'] = lrtu_df['Yearly Journeys'].str.replace(\n",
    "        ',', '')\n",
    "    lrtu_df['Yearly Journeys'] = lrtu_df['Yearly Journeys'].astype(float) * 10 # Just clear float to minimise rounding error risk\n",
    "    lrtu_df['Yearly Journeys'] = lrtu_df['Yearly Journeys'].astype(int)\n",
    "    lrtu_df['Yearly Journeys'] = lrtu_df['Yearly Journeys'] * 100000 # Not 1 million as we've times by 10 about to get out of float\n",
    "\n",
    "    # Apply sectors to data\n",
    "    lrtu_df['Sector'] = lrtu_df['System'].map(lrtu_systems)\n",
    "    lrtu_df = lrtu_df.dropna(axis=0) # Drop rows where system name is not found (expected to be some total rows like all of GB)\n",
    "    if lrtu_df.shape[0] != len(lrtu_systems):\n",
    "        print('WARNING: The systems you have specified sectors for and the systems in the input file do not match!')\n",
    "    lrtu_df = lrtu_df.groupby(\n",
    "        ['Sector'])['Yearly Journeys'].sum().reset_index()\n",
    "\n",
    "    # Apply scaling factors to account for overlap with other rail modes\n",
    "    # (e.g. national rail, other light rail systems)\n",
    "    lrtu_df['Yearly Journeys'] = lrtu_df['Yearly Journeys'] * np.where(\n",
    "        lrtu_df['Sector'] == 'London', lrtu_london_scale, lrtu_nonlondon_scale)\n",
    "    lrtu_df['Yearly Journeys'] = lrtu_df['Yearly Journeys'].astype(int)\n",
    "    \n",
    "    return lrtu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rationalise_inputs(odm_df, msoa_county_df, stn_geo_df):\n",
    "    \"\"\"\n",
    "    Cut input dfs down to just the columns of interest\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    odm_df: pandas df\n",
    "        Origin-destination matrix for journeys on the national rail network\n",
    "        between station pairs. Annual data for 1 year\n",
    "    msoa_county_df: pandas df\n",
    "        Lookup table to get from MSOA to County\n",
    "    stn_geo_df: pandas df\n",
    "        National rail stations with MSOA attached\n",
    "    \n",
    "    Returns\n",
    "    ---------- \n",
    "    odm_df: pandas df\n",
    "        Origin-destination matrix for journeys on the national rail network\n",
    "        between station pairs. Annual data for 1 year. Columns cut down to just\n",
    "        those required by other functions\n",
    "    msoa_county_df: pandas df\n",
    "        Lookup table to get from MSOA to County. Columns cut down to just those\n",
    "        required by other functions\n",
    "    stn_geo_df: pandas df\n",
    "        National rail stations with MSOA attached. Columns cut down to just\n",
    "        those required by other functions\n",
    "    \"\"\"\n",
    "    \n",
    "    # nlc (National Location Code) is a unique numerical code for each station\n",
    "    odm_df = odm_df[['origin_nlc',\n",
    "                     'origin_station_name',\n",
    "                     'destination_nlc',\n",
    "                     'destination_station_name',\n",
    "                     'journeys']]\n",
    "    msoa_county_df = msoa_county_df[['msoa11cd',\n",
    "                                     'county',\n",
    "                                     'county_nm']]\n",
    "    stn_geo_df = stn_geo_df[['National Location Code', 'msoa11cd']]\n",
    "    \n",
    "    return odm_df, msoa_county_df, stn_geo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_station_geography(msoa_county_df, stn_geo_df, stn_infill_df):\n",
    "    \"\"\"\n",
    "    Join each national rail station to the county in which they lie\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    msoa_county_df: pandas df\n",
    "        Lookup table to get from MSOA to County. Columns cut down to just those\n",
    "        required by this function\n",
    "    stn_geo_df: pandas df\n",
    "        National rail stations with MSOA attached. Columns cut down to just\n",
    "        those required by this function\n",
    "    stn_infill_df: pandas df\n",
    "        Table assigning stations outside of MSOAs to their counties\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    stn_geo_df: pandas df\n",
    "        Table relating all active national rail stations to their county\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assign counties to stations that are allocated MSOAs by the geospatial\n",
    "    # processing\n",
    "    stn_geo_df = stn_geo_df.merge(msoa_county_df, how='left', on='msoa11cd')\n",
    "    stn_geo_df = stn_geo_df.drop(columns=['msoa11cd'], axis=1)\n",
    "    \n",
    "    # Add on the stations that exist outside of the MSOA shapefile\n",
    "    # Drop rows containing nulls\n",
    "    if stn_geo_df[stn_geo_df.isnull().any(axis=1)].shape == stn_county_infill_df.shape:\n",
    "        # We are infilling something the same size as the NULL rows,\n",
    "        # which we want to do\n",
    "        # Drop the NULL rows, then append the replacements\n",
    "        stn_geo_df = stn_geo_df.dropna(how='any', axis=0)\n",
    "        stn_geo_df = pd.concat([stn_geo_df, stn_county_infill_df])\n",
    "        stn_geo_df.reset_index(inplace=True, drop=True)\n",
    "    else:\n",
    "        print('WARNING: The NULL infilling table you are trying to append is not the same dimensions as the NULL rows in the table')\n",
    "        print('Operation therefore not attempted and NULL rows are still in place')\n",
    "    \n",
    "    # Rename the National Location Code to make it a bit less unweildly\n",
    "    stn_geo_df = stn_geo_df.rename(columns={'National Location Code': 'nlc'})\n",
    "    \n",
    "    return stn_geo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sector_rail_odm(process_odm_df, stn_geo_df, sector_df):\n",
    "    \"\"\"\n",
    "    Make the sectorised national rail odm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    process_odm_df: pandas df\n",
    "        Origin-destination matrix for journeys on the national rail network\n",
    "        between station pairs. Annual data for 1 year. Columns cut down to just\n",
    "        those required by this function\n",
    "    stn_geo_df: pandas df\n",
    "        Table relating all active national rail stations to their county\n",
    "    sector_df: pandas df\n",
    "        County to sector correspondence\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    sector_odm_df: pandas df\n",
    "        Column matrix of all sector origin-destination movemnets from the\n",
    "        national rail ticketing data\n",
    "    county_rows: int\n",
    "        Row count of county level matrix\n",
    "    sector_rows: int\n",
    "        Row count of sector level matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Join geography to ODM\n",
    "    process_odm_df = process_odm_df.merge(\n",
    "        stn_geo_df, how='left', left_on='origin_nlc', right_on='nlc')\n",
    "    process_odm_df = process_odm_df.drop(columns=['nlc'], axis=1)\n",
    "    process_odm_df = process_odm_df.rename(\n",
    "        columns={'county': 'origin_county_code',\n",
    "                 'county_nm': 'origin_county_name'})\n",
    "\n",
    "    process_odm_df = process_odm_df.merge(\n",
    "        stn_geo_df, how='left', left_on='destination_nlc', right_on='nlc')\n",
    "    process_odm_df = process_odm_df.drop(columns=['nlc'], axis=1)\n",
    "    process_odm_df = process_odm_df.rename(\n",
    "        columns={'county': 'destination_county_code',\n",
    "                 'county_nm': 'destination_county_name'})\n",
    "\n",
    "    # Groupby on county level geographies, summing journeys and dropping the\n",
    "    # station details\n",
    "    process_odm_df = process_odm_df.groupby(\n",
    "        ['origin_county_code',\n",
    "         'origin_county_name',\n",
    "         'destination_county_code',\n",
    "         'destination_county_name']\n",
    "    )['journeys'].sum().reset_index()\n",
    "\n",
    "    # Now aggregate to Sector level using the Tour Model Output County to\n",
    "    # sector correspondence\n",
    "    sector_df = sector_df[['county', 'Sector_ID', 'Sector']]\n",
    "    sector_odm_df = process_odm_df.merge(\n",
    "        sector_df, how='left', left_on='origin_county_name', right_on='county')\n",
    "    sector_odm_df = sector_odm_df.drop(columns=['county'], axis=1)\n",
    "    sector_odm_df = sector_odm_df.rename(\n",
    "        columns={'Sector_ID': 'origin_sector_id',\n",
    "                 'Sector': 'origin_sector_name'})\n",
    "\n",
    "    sector_odm_df = sector_odm_df.merge(\n",
    "        sector_df,\n",
    "        how='left',\n",
    "        left_on='destination_county_name',\n",
    "        right_on='county'\n",
    "    )\n",
    "    sector_odm_df = sector_odm_df.drop(columns=['county'], axis=1)\n",
    "    sector_odm_df = sector_odm_df.rename(\n",
    "        columns={'Sector_ID': 'destination_sector_id',\n",
    "                 'Sector': 'destination_sector_name'})\n",
    "\n",
    "    # Groupby on sector level geographies, summing journeys and dropping the\n",
    "    # county details\n",
    "    sector_odm_df = sector_odm_df.groupby(\n",
    "        ['origin_sector_id',\n",
    "         'origin_sector_name',\n",
    "         'destination_sector_id',\n",
    "         'destination_sector_name']\n",
    "    )['journeys'].sum().reset_index()\n",
    "    \n",
    "    # Calculate row counts for checking\n",
    "    county_rows = process_odm_df.shape[0]\n",
    "    sector_rows = sector_odm_df.shape[0]\n",
    "    \n",
    "    return sector_odm_df, county_rows, sector_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_odm_processing(\n",
    "    county_rows, sector_rows, msoa_county_df, sector_df, sector_odm_df, odm_df):\n",
    "    \"\"\"\n",
    "    Check the odm processing worked correctly\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    county_rows: int\n",
    "        Row count of county level matrix\n",
    "    sector_rows: int\n",
    "        Row count of sector level matrix\n",
    "    msoa_county_df: pandas df\n",
    "        Lookup table to get from MSOA to County\n",
    "    sector_odm_df: pandas df\n",
    "        Column matrix of all sector origin-destination movemnets from the\n",
    "        national rail ticketing data\n",
    "    odm_df: pandas df\n",
    "        Origin-destination matrix for journeys on the national rail network\n",
    "        between station pairs. Annual data for 1 year\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if we've got all o/d movements at the county level\n",
    "    # df should have number of counties x number of counties as the row count\n",
    "    expected_rows = msoa_county_df['county'].nunique() ** 2\n",
    "    if expected_rows == county_rows:\n",
    "        print('County table dimensions are as expected')\n",
    "    else:\n",
    "        print(f'WARNING: Expected {str(expected_rows)} rows in the county table, got {str(county_rows)} rows')\n",
    "\n",
    "    # Check if we've got all o/d movements at the sector level\n",
    "    # df should have number of sectors x number of sectors as the row count\n",
    "    expected_rows = sector_df['Sector_ID'].nunique() ** 2\n",
    "    if expected_rows == sector_rows:\n",
    "        print('Sector table dimensions are as expected')\n",
    "    else:\n",
    "        print(f'WARNING: Expected {str(expected_rows)} rows in the sector table, got {str(sector_rows)} rows')\n",
    "\n",
    "    # Check we've not dropped any journeys\n",
    "    input_journeys = odm_df['journeys'].sum()\n",
    "    output_journeys = sector_odm_df['journeys'].sum()\n",
    "    if input_journeys == output_journeys:\n",
    "        print(f'The {str(input_journeys)} National Rail journeys input are all accounted for')\n",
    "    else:\n",
    "        print(f'WARNING: {str(input_journeys)} were input, but {str(output_journeys)} were output in the sector table!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lrtu_to_national_rail(\n",
    "    sector_odm_hlr_df, lrtu_df):\n",
    "    \"\"\"\n",
    "    Add Light Rail, Tramway and Underground trips to intrasector cells of\n",
    "    the main o/d sector matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sector_odm_hlr_df: pandas df\n",
    "        Column matrix of all sector origin-destination movemnets from the\n",
    "        national rail ticketing data\n",
    "    lrtu_df: pandas df\n",
    "        For the selected year, the estimate of the number of \"unique\" (i.e. not\n",
    "        double counted with another rail mode) journeys by Light Rail, Tramway\n",
    "        and Underground for the sectors in which such systems are located.\n",
    "        This is an annual total\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    sector_odm_hlr_df: pandas df\n",
    "        Column matrix of all sector origin-destination movemnets from the\n",
    "        national rail ticketing data, with the light rail, tramway and\n",
    "        underground data joined on the related intrasector movements\n",
    "    square_s_odm_df: pandas df\n",
    "        Square matrix of all sector origin-destination movemnets from the\n",
    "        national rail ticketing data, with the light rail, tramway and\n",
    "        underground data joined on the related intrasector movements\n",
    "    square_s_weekly_odm_df: pandas df\n",
    "        Square matrix of all sector origin-destination movemnets from the\n",
    "        national rail ticketing data, with the light rail, tramway and\n",
    "        underground data joined on the related intrasector movements, scaled to\n",
    "        be weekly to match the Tour Model outputs\n",
    "    \"\"\"\n",
    "    hr_total = sector_odm_hlr_df['journeys'].sum()\n",
    "    lrtu_total = lrtu_df['Yearly Journeys'].sum()\n",
    "    \n",
    "    sector_odm_hlr_df['Sector'] = np.where(\n",
    "        sector_odm_hlr_df['origin_sector_id'] == sector_odm_hlr_df['destination_sector_id'],\n",
    "        sector_odm_hlr_df['origin_sector_name'], '-')\n",
    "    sector_odm_hlr_df = sector_odm_hlr_df.merge(\n",
    "        lrtu_df, how='left', on='Sector')\n",
    "    sector_odm_hlr_df['Yearly Journeys'] = sector_odm_hlr_df['Yearly Journeys'].fillna(0).astype(int)\n",
    "    sector_odm_hlr_df['journeys'] = (sector_odm_hlr_df['journeys'] +\n",
    "                                     sector_odm_hlr_df['Yearly Journeys'])\n",
    "    sector_odm_hlr_df = sector_odm_hlr_df.drop(\n",
    "        ['Sector', 'Yearly Journeys'], axis=1)\n",
    "\n",
    "    hrlrtu_total = sector_odm_hlr_df['journeys'].sum()\n",
    "    if hr_total + lrtu_total != hrlrtu_total:\n",
    "        print('WARNING: Unexpected mismatch between National Rail and other rail totals with their sum!')\n",
    "    \n",
    "    # Make square\n",
    "    square_s_odm_df = sector_odm_hlr_df.pivot(\n",
    "        index=['origin_sector_id', 'origin_sector_name'],\n",
    "        columns=['destination_sector_id', 'destination_sector_name'],\n",
    "        values='journeys'\n",
    "    )\n",
    "    \n",
    "    # Make square matrix weekly (to match Tour Model Output)\n",
    "    square_s_weekly_odm_df = square_s_odm_df/52\n",
    "    \n",
    "    return sector_odm_hlr_df, square_s_odm_df, square_s_weekly_odm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to process model output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_rail_mat(model_df, sector_df):\n",
    "    \"\"\"\n",
    "    Process the model outputs to get a square matrix that matches the format and\n",
    "    dimensions of the rail ticketing data matrix. Also produce a matrix for\n",
    "    scaling to subsets of the overall df later.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_input: pandas df\n",
    "        The county level o/d matrix in stack format exported by the tour model.\n",
    "        This contains additional modes, as well as trip purposes and time\n",
    "        periods\n",
    "    sector_df: pandas df\n",
    "        County to sector correspondence\n",
    "\n",
    "    Outputs\n",
    "    ----------\n",
    "    rail_mat: pandas df\n",
    "        Square rail matrix derived from the tour model. Set to the county level\n",
    "    pdt_props_df: pandas df\n",
    "        For each purpose, direction and time period combination, this df lists\n",
    "        the proportion of rail trips relative to all rail trips\n",
    "    \"\"\"\n",
    "    # Filter to rail only\n",
    "    rail_mat = model_df[model_df['mode'] == 'Rail']\n",
    "    sector_df = sector_df[['county', 'Sector', 'Sector_ID']]\n",
    "    \n",
    "    # Make purpose, directrion and time period proportions\n",
    "    pdt_props_df = rail_mat.groupby(['purpose',\n",
    "                                        'direction',\n",
    "                                        'period']\n",
    "                                      )['trips'].sum().reset_index()\n",
    "    rail_trip_tot = pdt_props_df['trips'].sum()\n",
    "    pdt_props_df['proportion'] = pdt_props_df['trips'] /  rail_trip_tot\n",
    "    pdt_props_df = pdt_props_df.drop(columns=['trips'])\n",
    "    \n",
    "    # Make square, county level tour model output rail matrix\n",
    "    rail_mat = rail_mat.rename(\n",
    "        columns={'tmz_o': 'county_origin', 'tmz_d': 'county_destination'})\n",
    "    for d in ['destination', 'origin']:\n",
    "        rail_mat = rail_mat.merge(sector_df,\n",
    "                                  left_on='_'.join(['county', d]),\n",
    "                                  right_on='county', how='left')\n",
    "        colname_s = '_'.join([d, 'sector_name'])\n",
    "        colname_si = '_'.join([d, 'sector_id'])\n",
    "        rail_mat = rail_mat.rename(columns={'Sector': colname_s,\n",
    "                                            'Sector_ID': colname_si})\n",
    "        rail_mat = rail_mat.drop(columns=['county'])\n",
    "    rail_mat = rail_mat[['destination_sector_name',\n",
    "                         'destination_sector_id',\n",
    "                         'origin_sector_name',\n",
    "                         'origin_sector_id',\n",
    "                         'trips']]\n",
    "    rail_mat = rail_mat.groupby(['destination_sector_name',\n",
    "                                 'destination_sector_id',\n",
    "                                 'origin_sector_name',\n",
    "                                 'origin_sector_id']\n",
    "                               )['trips'].sum().reset_index()\n",
    "    rail_mat = rail_mat.pivot(\n",
    "        index=['origin_sector_id', 'origin_sector_name'],\n",
    "        columns=['destination_sector_id', 'destination_sector_name'],\n",
    "        values='trips'\n",
    "        )\n",
    "    \n",
    "    rail_mat = rail_mat.fillna(0) # Fill missing data with 0 (i.e. no trips)\n",
    "    rail_mat = rail_mat.reindex(sorted(rail_mat.columns), axis=1)\n",
    "\n",
    "    return rail_mat, pdt_props_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "furness_params = {\n",
    "    'tolerance': 1e-9,\n",
    "    'max_iterations': 5000,\n",
    "    'warning': True\n",
    "}\n",
    "\n",
    "def furness_rail(ticket_mat, rail_mat, furness_params):\n",
    "    \"\"\"\n",
    "    Furness the tour model output rail matrix to get the patterns from the\n",
    "    ticketing data. Expect this to fill in the blanks (0s) in the rail matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ticket_mat: pandas df\n",
    "        Target matrix for the furness process. Extracted from rail ticketing\n",
    "        data\n",
    "    rail_mat: pandas df\n",
    "        Seed data for the furness process. These are the tour model output rail\n",
    "        trips\n",
    "    furness_params: dict\n",
    "        Contains tolerance, maximum iterations and whether warnings are shown by\n",
    "        the furness process\n",
    "\n",
    "    Outputs\n",
    "    ----------\n",
    "    f_mat_df: pandas df\n",
    "        Matrix output from the Furness process\n",
    "    ticket_mat: pandas df\n",
    "        The ticketing data matrix as seen by the furness process to enable\n",
    "        before and after comparison\n",
    "    rail_mat: pandas df\n",
    "        The tour model rail outputs as seen by the furness process to enable\n",
    "        before and after comparison\n",
    "    \"\"\"\n",
    "    # Scale ticketing data to same total as tour model output\n",
    "    ticket_tot = ticket_mat.sum().sum()\n",
    "    tour_tot = rail_mat.sum().sum()\n",
    "    ticket_mat = sq_s_weekly_odm_df * (tour_tot / ticket_tot)\n",
    "    \n",
    "    # Initialise furness process\n",
    "    # Check this is the correct way around...\n",
    "    row_targets = ticket_mat.sum(axis=0).reset_index(drop=True).to_numpy()\n",
    "    col_targets = ticket_mat.sum(axis=1).reset_index(drop=True).to_numpy()\n",
    "    seed_vals = rail_mat.replace(0, 1e-10).to_numpy()\n",
    "    \n",
    "    tol = furness_params['tolerance']\n",
    "    max_iters = furness_params['max_iterations']\n",
    "    warning = furness_params['warning']\n",
    "    \n",
    "    f_mat, iteration, rmse = furness.doubly_constrained_furness(seed_vals,\n",
    "                                                                row_targets,\n",
    "                                                                col_targets,\n",
    "                                                                tol,\n",
    "                                                                max_iters,\n",
    "                                                                warning)\n",
    "    print(f'Reached iteration {iteration} of {max_iters}')\n",
    "    print(f'With an RMSE of {rmse}')\n",
    "    \n",
    "    f_mat_df = pd.DataFrame(data=f_mat,\n",
    "                            index=ticket_mat.index,\n",
    "                            columns=ticket_mat.columns)\n",
    "\n",
    "    return f_mat_df, ticket_mat, rail_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_d_t_disagg(f_mat_lst, pdt_prop_mat):\n",
    "    \"\"\"\n",
    "    Convert the square matrix output from furness process to a column matrix.\n",
    "    Then apply propotions by purpose, direction and time period for each\n",
    "    purpose, direction and time period combination to calculate trips by\n",
    "    origin, destination, purpose, direction and time period.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_mat_lst: pandas df\n",
    "        Matrix output from the furness (in square format)\n",
    "    pdt_props_mat: pandas df\n",
    "        Proportions for each unique p, d, t combination output earlier in the\n",
    "        process to be allocated to the trips by o and d.\n",
    "\n",
    "    Outputs\n",
    "    ----------\n",
    "    dim_df: pandas df\n",
    "        Column matrix of every o, d, p, d, t combination with the trips by o, d\n",
    "        only; the p, d, t proportions; and the trips by o, d, p, d, t as\n",
    "        columns.\n",
    "    \"\"\"\n",
    "    in_trip_count = sq_furnessed_rail_matrix.sum().sum()\n",
    "    \n",
    "    # Return to column matrix\n",
    "    f_mat_lst = f_mat_lst.stack(\n",
    "        list(range(sq_furnessed_rail_matrix.columns.nlevels))).reset_index()\n",
    "    f_mat_lst = f_mat_lst.rename(columns={0: 'trips'})\n",
    "    \n",
    "    # Make a df containing all possible o, d, p, d, t combinations for m = rail\n",
    "    o_sec_names = list(f_mat_lst['origin_sector_name'].unique())\n",
    "    d_sec_names = list(f_mat_lst['destination_sector_name'].unique())\n",
    "    purps = list(pdt_prop_mat['purpose'].unique())\n",
    "    dirs = list(pdt_prop_mat['direction'].unique())\n",
    "    tps = list(pdt_prop_mat['period'].unique())\n",
    "    \n",
    "    dim_df = pd.DataFrame(product(o_sec_names, d_sec_names, purps, dirs, tps))\n",
    "    dim_df = dim_df.rename(columns={\n",
    "        0: 'origin_sector_name',\n",
    "        1: 'destination_sector_name',\n",
    "        2: 'purpose',\n",
    "        3: 'direction',\n",
    "        4: 'period'\n",
    "    })\n",
    "    # Output of this should have 97,344 rows, the product of:\n",
    "        # 26 origin sectors\n",
    "        # 26 destination sectors\n",
    "        # 8 purposes\n",
    "        # 3 directions\n",
    "        # 6 periods\n",
    "    \n",
    "    # Join furnessed rail data back to the df made above\n",
    "    dim_df = dim_df.merge(f_mat_lst,\n",
    "                          on=['origin_sector_name', 'destination_sector_name'],\n",
    "                          how='left'\n",
    "                         )\n",
    "    dim_df = dim_df[['origin_sector_id',\n",
    "                     'origin_sector_name',\n",
    "                     'destination_sector_id',\n",
    "                     'destination_sector_name',\n",
    "                     'purpose',\n",
    "                     'direction',\n",
    "                     'period',\n",
    "                     'trips']]\n",
    "    dim_df = dim_df.rename(columns={'trips': 'o_d_trips'})\n",
    "    \n",
    "    # Join p, d, t proportions to df made above\n",
    "    dim_df = dim_df.merge(pdt_prop_mat,\n",
    "                          on=['purpose', 'direction', 'period'],\n",
    "                          how='left'\n",
    "                         )\n",
    "    \n",
    "    # Add a column for o, d, p, d, t trips\n",
    "    dim_df['o_d_p_d_t_trips'] = dim_df['o_d_trips'] * dim_df['proportion']\n",
    "    \n",
    "    # Check trip count\n",
    "    out_trip_count = dim_df['o_d_p_d_t_trips'].sum()\n",
    "    print(f'{in_trip_count} trips were input in the square matrix by o and d only')\n",
    "    print(f'{out_trip_count} trips were output in the stack matrix by o, d, p, d and t')\n",
    "    print('An exact match here is difficult to obtain with float operations, so test for a diff of 1e-8 or less')\n",
    "    if abs(in_trip_count - out_trip_count) < 1e-8:\n",
    "        print('Trips input and output to the p, d ,t diagreation process match within the tolerance')\n",
    "    else:\n",
    "        print('WARNING: Trips input and output to the p, d ,t diagreation process match within the tolerance!')\n",
    "    \n",
    "    return dim_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Call functions to run process, then produce output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "County table dimensions are as expected\n",
      "Sector table dimensions are as expected\n",
      "The 1228517053 National Rail journeys input are all accounted for\n",
      "Reached iteration 473 of 5000\n",
      "With an RMSE of 9.78724485805892e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jimny\\AppData\\Local\\Temp\\ipykernel_9392\\640068197.py:26: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  f_mat_lst = f_mat_lst.stack(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31414476.23184446 trips were input in the square matrix by o and d only\n",
      "31414476.231844466 trips were output in the stack matrix by o, d, p, d and t\n",
      "An exact match here is difficult to obtain with float operations, so test for a diff of 1e-8 or less\n",
      "Trips input and output to the p, d ,t diagreation process match within the tolerance\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "lrtu_df_processed = process_lrtu_data(\n",
    "    lrtu_in_df,\n",
    "    lrtu_year_in,\n",
    "    lrtu_systems_in,\n",
    "    lrtu_london_scale_in,\n",
    "    lrtu_nonlondon_scale_in\n",
    ")\n",
    "odm_df_rat, msoa_county_df_rat, stn_geo_df_rat = rationalise_inputs(\n",
    "    odm_in_df,\n",
    "    msoa_county_in_df,\n",
    "    stn_geo_in_df\n",
    ")\n",
    "stn_geo_df_processed = process_station_geography(\n",
    "    msoa_county_df_rat,\n",
    "    stn_geo_df_rat,\n",
    "    stn_county_infill_df\n",
    ")\n",
    "sector_odm_df_processed, c_rows, s_rows = make_sector_rail_odm(\n",
    "    odm_df_rat,\n",
    "    stn_geo_df_processed,\n",
    "    sector_in_df\n",
    ")\n",
    "check_odm_processing(\n",
    "    c_rows,\n",
    "    s_rows,\n",
    "    msoa_county_df_rat,\n",
    "    sector_in_df,\n",
    "    sector_odm_df_processed,\n",
    "    odm_df_rat\n",
    ")\n",
    "s_odm_hlr_df, sq_s_odm_df, sq_s_weekly_odm_df = add_lrtu_to_national_rail(\n",
    "    sector_odm_df_processed,\n",
    "    lrtu_df_processed\n",
    ")\n",
    "sq_rail_mat, pdt_props = make_model_rail_mat(\n",
    "    model_in_df,\n",
    "    sector_in_df\n",
    ")\n",
    "sq_furnessed_rail_matrix, sq_ticket_mat, sq_rail_mat = furness_rail(\n",
    "    sq_s_weekly_odm_df,\n",
    "    sq_rail_mat,\n",
    "    furness_params\n",
    ")\n",
    "cl_furnessed_rail_matrix_by_pdt = p_d_t_disagg(\n",
    "    sq_furnessed_rail_matrix,\n",
    "    pdt_props\n",
    ")\n",
    "\n",
    "sq_furnessed_rail_matrix.to_csv(r'I:\\NTS\\outputs\\tour\\constrain_prototype_data\\rail\\v1\\01_Furnessed_Square_Rail_Matrix.csv')\n",
    "sq_ticket_mat.to_csv(r'I:\\NTS\\outputs\\tour\\constrain_prototype_data\\rail\\v1\\00_Square_Weekly_Ticket+_Data.csv')\n",
    "sq_rail_mat.to_csv(r'I:\\NTS\\outputs\\tour\\constrain_prototype_data\\rail\\v1\\00_Square_Tour_Model_Rail_Furness_Input.csv')\n",
    "cl_furnessed_rail_matrix_by_pdt.to_csv(r'I:\\NTS\\outputs\\tour\\constrain_prototype_data\\rail\\v1\\01_Furnessed_Column_Rail_Matrix_by_p_d_t.csv')\n",
    "\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
